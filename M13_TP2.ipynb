{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "UtZBXCxdajr1",
   "metadata": {
    "id": "UtZBXCxdajr1"
   },
   "source": [
    "**Diplomatura en Ciencia de Datos, Aprendizaje Automático y sus Aplicaciones**\n",
    "\n",
    "---\n",
    "\n",
    "Mentoría #13 - Cómo hacer un Clasificador de Pliegos todoterreno (y de otros tipos de textos) usando NLP\n",
    "\n",
    "---\n",
    "Integrantes Grupo [1|2]:\n",
    "*   Bruce Wayne: bruce.w@wayne-entreprises.com\n",
    "*   Alfred Pennyworth: alfred@msn.com\n",
    "*   Richard Grayson: robin_132@gmail.com\n",
    "---\n",
    "\n",
    "Edición 2023\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "YGFbz1dvbFI2",
   "metadata": {
    "id": "YGFbz1dvbFI2"
   },
   "source": [
    "# Introducción\n",
    "\n",
    "En esta Mentoría, trabajaremos con un conjunto de datos que comprende aproximadamente 100.000 pliegos y licitaciones de diversos organismos nacionales, tanto públicos como privados. Estos datos se obtuvieron de un sistema/servicio diseñado para monitorear oportunidades de negocios, capturar la información en una base de datos, normalizarla y, posteriormente, clasificarla para informar a los usuarios según sus áreas de interés. Los usuarios de este sistema reciben alertas automáticas cada vez que se publica una oportunidad comercial que coincide con su perfil.\n",
    "\n",
    "## Desafío\n",
    "\n",
    "Como parte del proceso de clasificación, los pliegos se etiquetan utilizando principalmente reglas estáticas, como palabras clave, lo cual deja margen para optimizaciones. El título de cada pliego y principalmente la descripción de los objetos que se están licitando son campos de tipo texto y escritos por personas, por lo que naturalmente presentan ambigüedades y características propias del lenguaje que llevan a que un enfoque rígido y estático de clasificación, como el actual, resulte limitado y poco eficiente.\n",
    "\n",
    "El Desafío es utilizar las técnicas de Aprendizaje Automático, logrando así un \"Clasificador\" que utilice técnicas de Procesamiento de Lenguaje Natural (NLP) para clasificar de manera más eficiente en qué rubro o categoría se encuentra un pliego, basándose en su texto descriptivo y otros campos relacionados.\n",
    "\n",
    "## Interés General\n",
    "\n",
    "Más allá de la aplicación específica en este conjunto de datos, este problema, al estar vinculado al Procesamiento de Lenguaje Natural (PLN) y la clasificación, tiene la ventaja de poder ser utilizado posteriormente para otros tipos de contenido. De esta manera, el clasificador desarrollado podría aplicarse para categorizar libros, noticias, textos, tweets, publicaciones, artículos, etc.\n",
    "\n",
    "## Descripción del dataset\n",
    "\n",
    "A continuación se enumeran las diferentes variables del dataset, así como una breve descripción de su significado:\n",
    "\n",
    "- **id**: Clave única y primaria autoincremental de la tabla;\n",
    "- **cargado**: Fecha de carga del pliego;\n",
    "- **idexterno**: Id del pliego en la fuente;\n",
    "- **referencia**: Campo auxiliar obtenido de la fuente;\n",
    "- **objeto**: Campo principal, descripción del producto o servicio objeto de la licitación;\n",
    "- **rubro**: Campo de categorización disponible en la fuente. No siempre está disponible;\n",
    "- **agencia**: Empresa o Ente que lanza la licitación;\n",
    "- **apertura**: Fecha de apertura del pliego (vencimiento para presentarse al pliego);\n",
    "- **subrubro**: Subcategoría del pliego (también obtenido desde la fuente);\n",
    "- **pais**: País donde se lanza la licitación;\n",
    "- **observaciones**: Campo auxiliar donde se guardan datos extra que puede variar según la fuente;\n",
    "- **monto**: Monto del pliego, no siempre está publicado;\n",
    "- **divisaSimboloISO**: Moneda en la que se especifica el pliego;\n",
    "- **visible**: campo binario que determina si el pliego va a ser visualizado por el sistema (True/False);\n",
    "- **categoría**: Tipo de pliego, categorización entre diversos tipos de pliegos (Compra Directa, Licitación Simple, Subasta, etc.);\n",
    "- **fuente**: Fuente de donde se obtuvo la licitación."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaImQfxeYd8Z",
   "metadata": {
    "id": "eaImQfxeYd8Z"
   },
   "source": [
    "# Trabajo Práctico #2\n",
    "\n",
    "Como vimos en el primer Trabajo Práctico, en esta segunda entrega tenemos como objetivo preparar los datos para poder alimentar un modelo de predicción. Aplicaremos diferentes tipos de prácticas y estrategias para preparar el dataset y resolver todas las cuestiones detectadas en el TP1\n",
    "\n",
    "A continuación se enumeran los ejercicios propuestos como guía:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "EXILjx8pd2zb",
   "metadata": {
    "id": "EXILjx8pd2zb"
   },
   "source": [
    "## Ejercicio 1: Limpieza de Datos\n",
    "\n",
    "Según lo vimos en el TP1, aplicar las técnicas propuesta para limpiar los datos. En todos los casos, además de aplicar la técnica, redactar brevemente la explicación del criterio tomado.\n",
    "\n",
    "1.   Selección de Variables. Como vimos, hay variables que pueden descartarse.\n",
    "2.   Corrección de los diferentes Tipos de Variables. Aplicar las conversiones de tipo necesarias.\n",
    "3.   Selección de Registros. Filtrar las filas que no utilizaremos para la clasificación.\n",
    "4.   Curación de la variable [monto]. Tip: usar la variable [divisaSimboloISO]\n",
    "5.   Curación de la variable [fecha]. Incluye la eliminación de Outliers.\n",
    "6.   En [agencia], normalizar si es que existen entradas con valores equivalentes.\n",
    "7.   (Opcional) Para las variables categóricas aplicar técnicas de encoding, de manera de ir transformando variables de texto en números que los algoritmos puedan procesar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rpbtM0BGkWQ4",
   "metadata": {
    "id": "rpbtM0BGkWQ4"
   },
   "source": [
    "## Ejercicio 2: Variable [rubro]\n",
    "\n",
    "1.   Unificar categorías (rubros) equivalentes entre distintas fuentes. Usar un criterio definido por el grupo y explicarlo en pocas palabras.\n",
    "2.   Utilizar visualizaciones que permitan ver todas las categorías y verificar que sean valores válidos.\n",
    "3.   En caso de haber registros con categorías inválidas, tomar algún criterio para imputar la categoría o eliminar el registro. Desarrollar la decisión tomada.\n",
    "4.   Resolver si se va a utilizar subrubro para aperturar la categorización en algún caso. Definir luego qué postura tomar con respecto a la variable [rubro]\n",
    "5.   En las casos en que existe más de una categoría, tomar alguna postura para que los registros finalmente queden con una única etiqueta."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07xb2chXeBjx",
   "metadata": {
    "id": "07xb2chXeBjx"
   },
   "source": [
    "## Ejercicio 3: Variable [objeto]\n",
    "\n",
    "1.   Aplicar técnicas de lematización para eliminar artículos, conectores, stop_words en general. Quitar acentos (tildes), mayúsculas, plurales, caractéres inválidos, abreviaturas, etc., dejando solo palabras importantes para la clasificación.\n",
    "2.   Utilizar técnicas de visualización para detectar otros términos muy frecuentes que no aportan valor a la clasificación.\n",
    "3.   (Opcional) Aplicar técnicas de tokenización para convertir la variable objeto en un array de tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2i0GCYnYhQE6",
   "metadata": {
    "id": "2i0GCYnYhQE6"
   },
   "source": [
    "## Ejercicio 4: Exportación\n",
    "\n",
    "Con el dataset ya transformado, generar una versión final y exportarla a un nuevo archivo. Este es el resultado final del TP2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fDj77JylhG3_",
   "metadata": {
    "id": "fDj77JylhG3_"
   },
   "source": [
    "## Ejercicio 5: Análisis\n",
    "\n",
    "En base a todo lo aprendido en las materias obligatorias cursadas, qué tipo de modelos de clasificación piensan que podrían aplicarse al dataset obtenido? Desarrollar brevemente una justificación en comparación con otros modelos posibles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b56da7",
   "metadata": {
    "id": "14b56da7"
   },
   "source": [
    "# Desarrollo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "XbgaB6JqjjWy",
   "metadata": {
    "id": "XbgaB6JqjjWy"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import missingno as msno\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "df = pd.read_csv(\"./datos/Mentoria_Dataset_0.csv\" , encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "BMcav1-2aaJP",
   "metadata": {
    "id": "BMcav1-2aaJP"
   },
   "outputs": [],
   "source": [
    "#en el practico 1 decidimos tirar los datos que tienen categoría 2:\n",
    "#tiro los que tienen categoria 2\n",
    "df_fil = df[df['categoria']==1]\n",
    "\n",
    "#variables que decidimos relevantes el práctico pasado: \n",
    "variables = ['referencia', 'objeto', 'rubro','agencia','apertura','monto','fuente']\n",
    "\n",
    "#filtro el dataframe con estas variables:\n",
    "df_fil = df_fil[variables]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2177b870",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hacemos el filtrado de fechas hecho en el práctico anterior:\n",
    "\n",
    "today_date = pd.Timestamp.today().date()\n",
    "min_date = pd.to_datetime('13/12/2015')\n",
    "df_fil.reset_index(inplace=True)  #reseteo indices, es util cuando indice=nro de fila\n",
    "\n",
    "#fechas malas que encontramos:\n",
    "fecha_mala1 = df_fil['apertura'][62]\n",
    "fecha_mala2 = df_fil['apertura'][2025]\n",
    "fecha_mala3 = df_fil['apertura'][11907]\n",
    "\n",
    "#removemos las fechas malas\n",
    "df_fil = df_fil.drop(df_fil[df_fil['apertura'] == fecha_mala1].index)\n",
    "df_fil = df_fil.drop(df_fil[df_fil['apertura'] == fecha_mala2].index)\n",
    "df_fil = df_fil.drop(df_fil[df_fil['apertura'] == fecha_mala3].index)\n",
    "\n",
    "#convierto a formato datetime\n",
    "df_fil['apertura'] = pd.to_datetime(df_fil['apertura'],format='%d/%m/%Y %H:%M')\n",
    "df_fil['apertura'] = df_fil['apertura'].dt.tz_localize(None)\n",
    "df_fil['apertura'] = df_fil['apertura'].dt.date\n",
    "\n",
    "#removemos outliers\n",
    "df_fil = df_fil.drop(df_fil[df_fil['apertura'] > today_date].index)\n",
    "df_fil = df_fil.drop(df_fil[pd.to_datetime(df_fil['apertura']) < min_date].index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d7564e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#precio del dolar oficial\n",
    "dolar_value = 255\n",
    "\n",
    "df_fil['monto'] = df_fil['monto'].replace(r'\\N', np.nan)\n",
    "df_fil['monto'] = df_fil['monto'].replace('0', 0)\n",
    "\n",
    "#remuevo las strings de espacios, pesos y dolares donde las haya. Cambio la representacion de string a float\n",
    "j=0\n",
    "for monto, i in zip(df_fil['monto'],df_fil.index):\n",
    "    if \" \" in str(monto):\n",
    "        df_fil['monto'][i] = str(df_fil['monto'][i]).replace(' ','')\n",
    "    if \"$\" in str(monto):\n",
    "        df_fil['monto'][i] = str(df_fil['monto'][i]).replace('$','')\n",
    "    if \".\" in str(monto):\n",
    "        df_fil['monto'][i] = str(df_fil['monto'][i]).replace('.','')\n",
    "    if \",\" in str(monto):\n",
    "        df_fil['monto'][i] = str(df_fil['monto'][i]).replace(',','.')    \n",
    "    if \"U$S\" in str(monto):\n",
    "        df_fil['monto'][i] = str(df_fil['monto'][i]).replace('US','')\n",
    "        #convierto a pesos y reemplazo la entrada\n",
    "        df_fil['monto'][i] = float(df_fil['monto'][i]) * dolar_value  \n",
    "        j+=1\n",
    "    df_fil['monto'][i] = float(df_fil['monto'][i]) \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55a3965",
   "metadata": {},
   "source": [
    "Empiezo a hacer el ejercicio 3:\n",
    "\n",
    "a- Aplicar técnicas de lematización para eliminar artículos, conectores, stop_words en general. Quitar acentos (tildes), mayúsculas, plurales, caractéres inválidos, abreviaturas, etc., dejando solo palabras importantes para la clasificación.\n",
    "\n",
    "b- Utilizar técnicas de visualización para detectar otros términos muy frecuentes que no aportan valor a la clasificación.\n",
    "    \n",
    "c- (Opcional) Aplicar técnicas de tokenización para convertir la variable objeto en un array de tokens.\n",
    "\n",
    "\n",
    "donde aplicar la limpieza:\n",
    "* objeto\n",
    "* rubro\n",
    "* agencia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79244cf1",
   "metadata": {},
   "source": [
    "# Ejercicio 3: Variable [objeto]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e367118c",
   "metadata": {},
   "source": [
    "## Ejercicio 3a: limpieza\n",
    "\n",
    "Empecemos viendo de forma general la distribución de palabras que hay en la variable **objeto**, viendo las primeras entradas del dataset para tener una idea. \n",
    "\n",
    "Notamos que:\n",
    "* Las entradas están llenas de signos de puntiación,\n",
    "* hay muchos artículos y conectores,\n",
    "* muchas palabras repetidas por estar algunas en mayúsculas, otras en minúsculas, y otras pueden estar abreviadas.\n",
    "* muchas palabras son las mismas y sólo cambian si están en singular y plural.\n",
    "\n",
    "Para tener una idea más detallada, usamos la librería `wordcloud`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6ae320",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import wordcloud\n",
    "\n",
    "#defino una función para plotear cloudwords:\n",
    "def plot_cloudword(data,name):\n",
    "    entire_words = \"\".join(item for item in str(data))\n",
    "    fig=plt.figure(figsize=(10,4))\n",
    "    ax=fig.add_subplot(111)\n",
    "    wc = wordcloud.WordCloud(background_color=\"white\",max_words=500,\n",
    "                          max_font_size=300,\n",
    "                          width=1920, \n",
    "                          height=1080,).generate(entire_words)\n",
    "    ax.imshow(wc, interpolation='bilinear')\n",
    "    ax.axis(\"off\")\n",
    "    ax.margins(x=0, y=0)\n",
    "    plt.savefig(name,dpi=300)\n",
    "#-------------------------------------------------------------------\n",
    "df_fil.dropna(inplace=True)    #sigue habiendo NaNs, los elimino\n",
    "#df_fil.reset_index(inplace=True)\n",
    "\n",
    "print(df_fil['objeto'][:100])\n",
    "\n",
    "plot_cloudword(df_fil['objeto'],'palabras_inicio')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4e54cd",
   "metadata": {},
   "source": [
    "Veamos cuántas palabras distintas hay en este dataset inicial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77b6b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "entire_words = \"\".join(item for item in df_fil['objeto'])\n",
    "list0 = entire_words.split()\n",
    "dfdf_dum2 = pd.DataFrame(list0,columns=['palabras'])\n",
    "\n",
    "counts = dfdf_dum2.value_counts()\n",
    "nwords = len(dfdf_dum2)\n",
    "print('cantidad de palabras en monto:',nwords)\n",
    "print('cantidad de palabras distintas:',len(counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d052b9",
   "metadata": {},
   "source": [
    "Lo primero que hacemos para limpiar la variable **objeto** es pasar todas las palabras a minúsculas. También cambiamos algunos espacios dobles o triples por espacios simples. Al hacer esto, pueden quedar espacios dobles o triples. Convertimos todo a espacios simples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f3eec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fil.reset_index(drop=True,inplace=True)\n",
    "\n",
    "table = str.maketrans('', '', string.punctuation) #tabla para eliminar signos de puntuacion\n",
    "\n",
    "#paso todo a minúsculas\n",
    "i=0\n",
    "for sentence in df_fil['objeto']:    \n",
    "    df_fil.loc[i,'objeto'] = str(df_fil['objeto'][i]).lower()\n",
    "    i+=1\n",
    "i=0\n",
    "for sentence in df_fil['objeto']:    \n",
    "    df_fil.loc[i,'objeto'] = str(df_fil['objeto'][i]).translate(table) #elimino signos de puntuacion\n",
    "    i+=1\n",
    "    \n",
    "#a veces, después de eliminar algunas palabras quedan espacios dobles o triples. Cambio a un espacio:\n",
    "df_fil['objeto'] = df_fil['objeto'].replace('   ',' ')\n",
    "df_fil['objeto'] = df_fil['objeto'].replace('  ',' ')\n",
    "\n",
    "#veamos cómo va quedando:\n",
    "print(df_fil['objeto'][:100])\n",
    "plot_cloudword(df_fil['objeto'],'palabras_min_no_punt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7cac7f5",
   "metadata": {},
   "source": [
    "Ahora que tenemos todo en minúsculas y sin signos, eliminamos tildes, artículos, preposiciones y otras palabras que creemos no aportan información relevante al pliego:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7e9c76",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "articulos = ['el','la','los','las','un','uno','una','unos','unas','lo','al','del']\n",
    "preposiciones = ['a', 'ante', 'bajo', 'cabe', 'con', 'contra', 'de', 'desde', 'durante',\n",
    "                 'en', 'entre', 'hacia', 'hasta', 'mediante', 'para', 'por', 'segun', \n",
    "                 'sin', 'so', 'sobre', 'tras', 'versus', 'via']\n",
    "otras = ['y','su','de','sus','n°']\n",
    "\n",
    "df_fil.reset_index(drop=True,inplace=True)\n",
    "\n",
    "# remuevo tildes y elimino palabras y algunos signos que no se eliminaron antes en un solo loop:\n",
    "i=0\n",
    "for sentence in df_fil['objeto']:  \n",
    "    #aca saco tildes, puntos suspensivos y el caracter de grado\n",
    "    sent = str(sentence)   # variable a analizar\n",
    "    if \"á\" in sent:\n",
    "        df_fil.loc[i,'objeto'] = df_fil.loc[i,'objeto'].replace('á','a')\n",
    "    if \"é\" in sent:\n",
    "        df_fil.loc[i,'objeto'] = df_fil.loc[i,'objeto'].replace('é','e')\n",
    "    if \"í\" in sent:\n",
    "        df_fil.loc[i,'objeto'] = df_fil.loc[i,'objeto'].replace('í','i')\n",
    "    if \"ó\" in sent:\n",
    "        df_fil.loc[i,'objeto'] = df_fil.loc[i,'objeto'].replace('ó','o')\n",
    "    if \"ú\" in sent:\n",
    "        df_fil.loc[i,'objeto'] = df_fil.loc[i,'objeto'].replace('ú','u')\n",
    "    if \"...\" in sent:\n",
    "        df_fil.loc[i,'objeto'] = df_fil.loc[i,'objeto'].replace('...','')\n",
    "    if \"°\" in sent:\n",
    "        df_fil.loc[i,'objeto'] = df_fil.loc[i,'objeto'].replace('°','')\n",
    "\n",
    "        \n",
    "    #ahora empiezo a eliminar palabras que no aportan nada de información.\n",
    "    #separo la oración en las palabras constituyentes\n",
    "    words = df_fil['objeto'][i].split()\n",
    " \n",
    "    #elimino artículos\n",
    "    cambie_algo = False\n",
    "    for art in articulos:\n",
    "        if art in words:\n",
    "            words.remove(art)\n",
    "            cambie_algo = True\n",
    "    #elimino preposiciones\n",
    "    for prep in preposiciones:\n",
    "        if prep in words:\n",
    "            words.remove(prep)\n",
    "            cambie_algo = True\n",
    "    #elimino otras\n",
    "    for otra in otras:\n",
    "        if otra in words:\n",
    "            words.remove(otra)\n",
    "            cambie_algo = True\n",
    "    if cambie_algo == True:\n",
    "        df_fil['objeto'][i] = \" \".join(words)\n",
    "    i+=1\n",
    "\n",
    "#veamos cómo va quedando:\n",
    "print(df_fil['objeto'][:100])\n",
    "plot_cloudword(df_fil['objeto'],'palabras_min_no_tildes')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b9ee46",
   "metadata": {},
   "source": [
    "El dataset va quedando más limpio, aunque hay algunas palabras que se repiten en la cloudword. Aplicamos una lematización a la variable objeto usando la librería `nltk`.\n",
    "\n",
    "En nlp, 'lematizar' es simplificar una palabra a otra que contenga la información base, como abreviar. Nos sirve para juntar palabras en distintos tiempos y personas por ejemplo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9772957f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords') #creo que estamos haciendo lo mismo que al eliminar las preposiciones con esto\n",
    "nltk.download('wordnet')\n",
    "\n",
    "#funcion para lematizar una palabra\n",
    "def lemmatize_word(word):\n",
    "    stemmer = SnowballStemmer('spanish')\n",
    "    return stemmer.stem(word)\n",
    "\n",
    "df_fil0 = df_fil.copy()\n",
    "\n",
    "print('Tokenizamos las oraciones en palabras')\n",
    "print()\n",
    "df_fil0['objeto'] = df_fil0['objeto'].apply(word_tokenize)\n",
    "print(df_fil0['objeto'][:10])\n",
    "print('*'*100)\n",
    "print('Removemos stopwords (son como las contracciones?)')\n",
    "print()\n",
    "stop_words = set(stopwords.words('spanish'))\n",
    "df_fil0['objeto'] = df_fil0['objeto'].apply(lambda words: [word for word in words if word not in stop_words])\n",
    "print(df_fil0['objeto'][:10])\n",
    "print('*'*100)\n",
    "print('Aplicamos lematización a cada palabra')\n",
    "print()\n",
    "df_fil0['objeto'] = df_fil0['objeto'].apply(lambda words: [lemmatize_word(word) for word in words])\n",
    "print(df_fil0['objeto'][:10])\n",
    "print('*'*100)\n",
    "\n",
    "# Unimos las palabras lematizadas en cada oración en otro dataset para ver cómo queda:\n",
    "df_dum = df_fil0['objeto'].apply(lambda words: ' '.join(words))\n",
    "#--------------------------------------------------------------\n",
    "#veamos cómo va quedando:\n",
    "print(df_dum[:10])\n",
    "plot_cloudword(df_dum,'palabras_min_no_tildes')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71d9690",
   "metadata": {},
   "source": [
    "veamos las distribución de palabras. En particular nos interesa qué porcentaje de los datos\n",
    "ocupan las palabras más frecuentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3b1c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "entire_words = \"\".join(item for item in df_dum)\n",
    "list0 = entire_words.split()\n",
    "dfdf_dum2 = pd.DataFrame(list0,columns=['palabras'])\n",
    "\n",
    "\n",
    "counts = dfdf_dum2.value_counts()\n",
    "nwords = len(dfdf_dum2)\n",
    "print('cantidad de palabras en monto:',nwords)\n",
    "print('cantidad de palabras distintas:',len(counts))\n",
    "print('las 1000 palabras más frecuentes son el '+str(round(np.sum(counts[:1000])/nwords*100,2))+'% del total')\n",
    "print('las 2000 palabras más frecuentes son el '+str(round(np.sum(counts[:2000])/nwords*100,2))+'% del total')\n",
    "print('las 5000 palabras más frecuentes son el '+str(round(np.sum(counts[:5000])/nwords*100,2))+'% del total')\n",
    "print('las 10000 palabras más frecuentes son el '+str(round(np.sum(counts[:10000])/nwords*100,2))+'% del total')\n",
    "print('las 20000 palabras más frecuentes son el '+str(round(np.sum(counts[:20000])/nwords*100,2))+'% del total')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287d32cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
