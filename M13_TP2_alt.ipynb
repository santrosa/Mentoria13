{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "UtZBXCxdajr1",
   "metadata": {
    "id": "UtZBXCxdajr1"
   },
   "source": [
    "**Diplomatura en Ciencia de Datos, Aprendizaje Automático y sus Aplicaciones**\n",
    "\n",
    "---\n",
    "\n",
    "Mentoría #13 - Cómo hacer un Clasificador de Pliegos todoterreno (y de otros tipos de textos) usando NLP\n",
    "\n",
    "---\n",
    "Integrantes Grupo [1|2]:\n",
    "*   Bruce Wayne: bruce.w@wayne-entreprises.com\n",
    "*   Alfred Pennyworth: alfred@msn.com\n",
    "*   Richard Grayson: robin_132@gmail.com\n",
    "---\n",
    "\n",
    "Edición 2023\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "YGFbz1dvbFI2",
   "metadata": {
    "id": "YGFbz1dvbFI2"
   },
   "source": [
    "# Introducción\n",
    "\n",
    "En esta Mentoría, trabajaremos con un conjunto de datos que comprende aproximadamente 100.000 pliegos y licitaciones de diversos organismos nacionales, tanto públicos como privados. Estos datos se obtuvieron de un sistema/servicio diseñado para monitorear oportunidades de negocios, capturar la información en una base de datos, normalizarla y, posteriormente, clasificarla para informar a los usuarios según sus áreas de interés. Los usuarios de este sistema reciben alertas automáticas cada vez que se publica una oportunidad comercial que coincide con su perfil.\n",
    "\n",
    "## Desafío\n",
    "\n",
    "Como parte del proceso de clasificación, los pliegos se etiquetan utilizando principalmente reglas estáticas, como palabras clave, lo cual deja margen para optimizaciones. El título de cada pliego y principalmente la descripción de los objetos que se están licitando son campos de tipo texto y escritos por personas, por lo que naturalmente presentan ambigüedades y características propias del lenguaje que llevan a que un enfoque rígido y estático de clasificación, como el actual, resulte limitado y poco eficiente.\n",
    "\n",
    "El Desafío es utilizar las técnicas de Aprendizaje Automático, logrando así un \"Clasificador\" que utilice técnicas de Procesamiento de Lenguaje Natural (NLP) para clasificar de manera más eficiente en qué rubro o categoría se encuentra un pliego, basándose en su texto descriptivo y otros campos relacionados.\n",
    "\n",
    "## Interés General\n",
    "\n",
    "Más allá de la aplicación específica en este conjunto de datos, este problema, al estar vinculado al Procesamiento de Lenguaje Natural (PLN) y la clasificación, tiene la ventaja de poder ser utilizado posteriormente para otros tipos de contenido. De esta manera, el clasificador desarrollado podría aplicarse para categorizar libros, noticias, textos, tweets, publicaciones, artículos, etc.\n",
    "\n",
    "## Descripción del dataset\n",
    "\n",
    "A continuación se enumeran las diferentes variables del dataset, así como una breve descripción de su significado:\n",
    "\n",
    "- **id**: Clave única y primaria autoincremental de la tabla;\n",
    "- **cargado**: Fecha de carga del pliego;\n",
    "- **idexterno**: Id del pliego en la fuente;\n",
    "- **referencia**: Campo auxiliar obtenido de la fuente;\n",
    "- **objeto**: Campo principal, descripción del producto o servicio objeto de la licitación;\n",
    "- **rubro**: Campo de categorización disponible en la fuente. No siempre está disponible;\n",
    "- **agencia**: Empresa o Ente que lanza la licitación;\n",
    "- **apertura**: Fecha de apertura del pliego (vencimiento para presentarse al pliego);\n",
    "- **subrubro**: Subcategoría del pliego (también obtenido desde la fuente);\n",
    "- **pais**: País donde se lanza la licitación;\n",
    "- **observaciones**: Campo auxiliar donde se guardan datos extra que puede variar según la fuente;\n",
    "- **monto**: Monto del pliego, no siempre está publicado;\n",
    "- **divisaSimboloISO**: Moneda en la que se especifica el pliego;\n",
    "- **visible**: campo binario que determina si el pliego va a ser visualizado por el sistema (True/False);\n",
    "- **categoría**: Tipo de pliego, categorización entre diversos tipos de pliegos (Compra Directa, Licitación Simple, Subasta, etc.);\n",
    "- **fuente**: Fuente de donde se obtuvo la licitación."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaImQfxeYd8Z",
   "metadata": {
    "id": "eaImQfxeYd8Z"
   },
   "source": [
    "# Trabajo Práctico #2\n",
    "\n",
    "Como vimos en el primer Trabajo Práctico, en esta segunda entrega tenemos como objetivo preparar los datos para poder alimentar un modelo de predicción. Aplicaremos diferentes tipos de prácticas y estrategias para preparar el dataset y resolver todas las cuestiones detectadas en el TP1\n",
    "\n",
    "A continuación se enumeran los ejercicios propuestos como guía:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "EXILjx8pd2zb",
   "metadata": {
    "id": "EXILjx8pd2zb"
   },
   "source": [
    "## Ejercicio 1: Limpieza de Datos\n",
    "\n",
    "Según lo vimos en el TP1, aplicar las técnicas propuesta para limpiar los datos. En todos los casos, además de aplicar la técnica, redactar brevemente la explicación del criterio tomado.\n",
    "\n",
    "1.   Selección de Variables. Como vimos, hay variables que pueden descartarse.\n",
    "2.   Corrección de los diferentes Tipos de Variables. Aplicar las conversiones de tipo necesarias.\n",
    "3.   Selección de Registros. Filtrar las filas que no utilizaremos para la clasificación.\n",
    "4.   Curación de la variable [monto]. Tip: usar la variable [divisaSimboloISO]\n",
    "5.   Curación de la variable [fecha]. Incluye la eliminación de Outliers.\n",
    "6.   En [agencia], normalizar si es que existen entradas con valores equivalentes.\n",
    "7.   (Opcional) Para las variables categóricas aplicar técnicas de encoding, de manera de ir transformando variables de texto en números que los algoritmos puedan procesar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rpbtM0BGkWQ4",
   "metadata": {
    "id": "rpbtM0BGkWQ4"
   },
   "source": [
    "## Ejercicio 2: Variable [rubro]\n",
    "\n",
    "1.   Unificar categorías (rubros) equivalentes entre distintas fuentes. Usar un criterio definido por el grupo y explicarlo en pocas palabras.\n",
    "2.   Utilizar visualizaciones que permitan ver todas las categorías y verificar que sean valores válidos.\n",
    "3.   En caso de haber registros con categorías inválidas, tomar algún criterio para imputar la categoría o eliminar el registro. Desarrollar la decisión tomada.\n",
    "4.   Resolver si se va a utilizar subrubro para aperturar la categorización en algún caso. Definir luego qué postura tomar con respecto a la variable [rubro]\n",
    "5.   En las casos en que existe más de una categoría, tomar alguna postura para que los registros finalmente queden con una única etiqueta."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07xb2chXeBjx",
   "metadata": {
    "id": "07xb2chXeBjx"
   },
   "source": [
    "## Ejercicio 3: Variable [objeto]\n",
    "\n",
    "1.   Aplicar técnicas de lematización para eliminar artículos, conectores, stop_words en general. Quitar acentos (tildes), mayúsculas, plurales, caractéres inválidos, abreviaturas, etc., dejando solo palabras importantes para la clasificación.\n",
    "2.   Utilizar técnicas de visualización para detectar otros términos muy frecuentes que no aportan valor a la clasificación.\n",
    "3.   (Opcional) Aplicar técnicas de tokenización para convertir la variable objeto en un array de tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2i0GCYnYhQE6",
   "metadata": {
    "id": "2i0GCYnYhQE6"
   },
   "source": [
    "## Ejercicio 4: Exportación\n",
    "\n",
    "Con el dataset ya transformado, generar una versión final y exportarla a un nuevo archivo. Este es el resultado final del TP2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fDj77JylhG3_",
   "metadata": {
    "id": "fDj77JylhG3_"
   },
   "source": [
    "## Ejercicio 5: Análisis\n",
    "\n",
    "En base a todo lo aprendido en las materias obligatorias cursadas, qué tipo de modelos de clasificación piensan que podrían aplicarse al dataset obtenido? Desarrollar brevemente una justificación en comparación con otros modelos posibles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b56da7",
   "metadata": {
    "id": "14b56da7"
   },
   "source": [
    "# Desarrollo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "XbgaB6JqjjWy",
   "metadata": {
    "id": "XbgaB6JqjjWy"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import missingno as msno\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "df = pd.read_csv(\"./datos/Mentoria_Dataset_0.csv\" , encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "BMcav1-2aaJP",
   "metadata": {
    "id": "BMcav1-2aaJP"
   },
   "outputs": [],
   "source": [
    "#en el practico 1 decidimos tirar los datos que tienen categoría 2:\n",
    "#tiro los que tienen categoria 2\n",
    "df_fil = df[df['categoria']==1]\n",
    "\n",
    "#variables que decidimos relevantes el práctico pasado: \n",
    "variables = ['referencia', 'objeto', 'rubro','agencia','apertura','monto','fuente']\n",
    "\n",
    "#filtro el dataframe con estas variables:\n",
    "df_fil = df_fil[variables]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2177b870",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hacemos el filtrado de fechas hecho en el práctico anterior:\n",
    "\n",
    "today_date = pd.Timestamp.today().date()\n",
    "min_date = pd.to_datetime('13/12/2015')\n",
    "df_fil.reset_index(inplace=True)  #reseteo indices, es util cuando indice=nro de fila\n",
    "\n",
    "#fechas malas que encontramos:\n",
    "fecha_mala1 = df_fil['apertura'][62]\n",
    "fecha_mala2 = df_fil['apertura'][2025]\n",
    "fecha_mala3 = df_fil['apertura'][11907]\n",
    "\n",
    "#removemos las fechas malas\n",
    "df_fil = df_fil.drop(df_fil[df_fil['apertura'] == fecha_mala1].index)\n",
    "df_fil = df_fil.drop(df_fil[df_fil['apertura'] == fecha_mala2].index)\n",
    "df_fil = df_fil.drop(df_fil[df_fil['apertura'] == fecha_mala3].index)\n",
    "\n",
    "#convierto a formato datetime\n",
    "df_fil['apertura'] = pd.to_datetime(df_fil['apertura'],format='%d/%m/%Y %H:%M')\n",
    "df_fil['apertura'] = df_fil['apertura'].dt.tz_localize(None)\n",
    "df_fil['apertura'] = df_fil['apertura'].dt.date\n",
    "\n",
    "#removemos outliers\n",
    "df_fil = df_fil.drop(df_fil[df_fil['apertura'] > today_date].index)\n",
    "df_fil = df_fil.drop(df_fil[pd.to_datetime(df_fil['apertura']) < min_date].index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d7564e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#precio del dolar oficial\n",
    "dolar_value = 255\n",
    "\n",
    "df_fil['monto'] = df_fil['monto'].replace(r'\\N', np.nan)\n",
    "df_fil['monto'] = df_fil['monto'].replace('0', 0)\n",
    "\n",
    "#remuevo las strings de espacios, pesos y dolares donde las haya. Cambio la representacion de string a float\n",
    "j=0\n",
    "for monto, i in zip(df_fil['monto'],df_fil.index):\n",
    "    if \" \" in str(monto):\n",
    "        df_fil['monto'][i] = str(df_fil['monto'][i]).replace(' ','')\n",
    "    if \"$\" in str(monto):\n",
    "        df_fil['monto'][i] = str(df_fil['monto'][i]).replace('$','')\n",
    "    if \".\" in str(monto):\n",
    "        df_fil['monto'][i] = str(df_fil['monto'][i]).replace('.','')\n",
    "    if \",\" in str(monto):\n",
    "        df_fil['monto'][i] = str(df_fil['monto'][i]).replace(',','.')    \n",
    "    if \"U$S\" in str(monto):\n",
    "        df_fil['monto'][i] = str(df_fil['monto'][i]).replace('US','')\n",
    "        #convierto a pesos y reemplazo la entrada\n",
    "        df_fil['monto'][i] = float(df_fil['monto'][i]) * dolar_value  \n",
    "        j+=1\n",
    "    df_fil['monto'][i] = float(df_fil['monto'][i]) \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55a3965",
   "metadata": {},
   "source": [
    "Empiezo a hacer el ejercicio 3:\n",
    "\n",
    "a- Aplicar técnicas de lematización para eliminar artículos, conectores, stop_words en general. Quitar acentos (tildes), mayúsculas, plurales, caractéres inválidos, abreviaturas, etc., dejando solo palabras importantes para la clasificación.\n",
    "\n",
    "b- Utilizar técnicas de visualización para detectar otros términos muy frecuentes que no aportan valor a la clasificación.\n",
    "    \n",
    "c- (Opcional) Aplicar técnicas de tokenización para convertir la variable objeto en un array de tokens.\n",
    "\n",
    "\n",
    "donde aplicar la limpieza:\n",
    "* objeto\n",
    "* rubro\n",
    "* agencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7e9c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "#tabla para reemplazar signos de puntuacion\n",
    "table_punct = str.maketrans(' ', ' ', string.punctuation)\n",
    "#tabla para reemplazar numeros\n",
    "table_num = str.maketrans(' ', ' ', '1234567890')\n",
    "table_otros = str.maketrans(' ', ' ', 'º°¿')\n",
    "\n",
    "#defino los datasets a limpiar\n",
    "df_test = df_fil[['objeto','rubro']]\n",
    "print(df_test['objeto'][0:100])\n",
    "\n",
    "df_test.reset_index(inplace=True)\n",
    "\n",
    "i=0\n",
    "for i in range (len(df_test)):    \n",
    "#for i in range (100):    \n",
    "    df_test['objeto'][i] = str(df_test['objeto'][i]).lower()\n",
    "    df_test['objeto'][i] = str(df_test['objeto'][i]).translate(table_punct) #elimino signos de puntuacion\n",
    "    df_test['objeto'][i] = str(df_test['objeto'][i]).translate(table_num) #elimino numeros\n",
    "    df_test['objeto'][i] = str(df_test['objeto'][i]).translate(table_otros) #elimino acentos y otros\n",
    "    \n",
    "    sent = str(df_test['objeto'][i])   # variable a analizar\n",
    "    if \"á\" in sent:\n",
    "        df_test['objeto'][i] = df_test['objeto'][i].replace('á','a')\n",
    "    if \"é\" in sent:\n",
    "        df_test['objeto'][i] = df_test['objeto'][i].replace('é','e')\n",
    "    if \"í\" in sent:\n",
    "        df_test['objeto'][i] = df_test['objeto'][i].replace('í','i')\n",
    "    if \"ó\" in sent:\n",
    "        df_test['objeto'][i] = df_test['objeto'][i].replace('ó','o')\n",
    "    if \"ú\" in sent:\n",
    "        df_test['objeto'][i] = df_test['objeto'][i].replace('ú','u')\n",
    "    if \"à\" in sent:\n",
    "        df_test['objeto'][i] = df_test['objeto'][i].replace('à','u')\n",
    "    if \"è\" in sent:\n",
    "        df_test['objeto'][i] = df_test['objeto'][i].replace('è','u')\n",
    "    if \"ì\" in sent:\n",
    "        df_test['objeto'][i] = df_test['objeto'][i].replace('ì','u')\n",
    "    if \"ò\" in sent:\n",
    "        df_test['objeto'][i] = df_test['objeto'][i].replace('ò','u')\n",
    "    if \"ù\" in sent:\n",
    "        df_test['objeto'][i] = df_test['objeto'][i].replace('ù','u')\n",
    "    i+=1\n",
    "    \n",
    "print(df_test['objeto'][0:100])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1f285b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "#funcion para lematizar una palabra\n",
    "def lemmatize_word(word):\n",
    "    stemmer = SnowballStemmer('spanish')\n",
    "    return stemmer.stem(word)\n",
    "\n",
    "#--------------------------------------------------------------------------------------\n",
    "df_test0 = df_test.copy()\n",
    "\n",
    "print('Tokenizamos las oraciones en palabras')\n",
    "print(df_test0['objeto'][0:1000])\n",
    "df_test0['objeto'] = df_test0['objeto'].apply(word_tokenize)\n",
    "print(df_test0['objeto'][:1000])\n",
    "print('*'*100)\n",
    "print('Removemos stopwords ')\n",
    "print()\n",
    "stop_words = set(stopwords.words('spanish'))\n",
    "df_test0['objeto'] = df_test0['objeto'].apply(lambda words: [word for word in words if word not in stop_words])\n",
    "print(df_test0['objeto'][:10])\n",
    "print('*'*100)\n",
    "print('Removemos palabras que creemos no aporta nada al dataset')\n",
    "adq_list = ['adquisicion','adq','adquis', 'adquirir', 'adquisiion',\n",
    "            'contrato','contratacion', 'compra', 'provision',\n",
    "            'servicio','alquiler','alq','subasta', 'locacion',\n",
    "            'monto','estimado', 'renovacion','estimado'] #elimino todo vestigio de adquisicion\n",
    "df_test0['objeto'] = df_test0['objeto'].apply(lambda words: [word for word in words if word not in adq_list])\n",
    "print(df_test0['objeto'][:10])\n",
    "print('*'*100)\n",
    "print('Aplicamos lematización a cada palabra')\n",
    "print()\n",
    "df_test0['objeto'] = df_test0['objeto'].apply(lambda words: [lemmatize_word(word) for word in words])\n",
    "print(df_test0['objeto'][:10])\n",
    "print('*'*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de9e134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unimos las palabras lematizadas en cada oración en otro dataset para ver cómo queda:\n",
    "df_dum = df_test0['objeto'].apply(lambda words: ' '.join(words))\n",
    "\n",
    "#print(df_dum.iloc[:100])\n",
    "\n",
    "entire_words = \"\".join(' '+item for item in df_dum) #el espacio ' '+item es fundamental para que no pegue palabras\n",
    "\n",
    "list0 = entire_words.split()\n",
    "#print(list0[10000:11000])\n",
    "\n",
    "df_dum2 = pd.DataFrame(list0,columns=['palabras'])\n",
    "\n",
    "counts = df_dum2.value_counts()\n",
    "print(counts[:1000])\n",
    "nwords = len(df_dum2)\n",
    "print('cantidad de palabras en monto:',nwords)\n",
    "print('cantidad de palabras distintas:',len(counts))\n",
    "print('las 1000 palabras más frecuentes son el '+str(round(np.sum(counts[:1000])/nwords*100,2))+'% del total')\n",
    "print('las 2000 palabras más frecuentes son el '+str(round(np.sum(counts[:2000])/nwords*100,2))+'% del total')\n",
    "print('las 5000 palabras más frecuentes son el '+str(round(np.sum(counts[:5000])/nwords*100,2))+'% del total')\n",
    "print('las 10000 palabras más frecuentes son el '+str(round(np.sum(counts[:10000])/nwords*100,2))+'% del total')\n",
    "print('las 20000 palabras más frecuentes son el '+str(round(np.sum(counts[:20000])/nwords*100,2))+'% del total')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
