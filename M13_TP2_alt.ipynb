{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "UtZBXCxdajr1",
   "metadata": {
    "id": "UtZBXCxdajr1"
   },
   "source": [
    "**Diplomatura en Ciencia de Datos, Aprendizaje Automático y sus Aplicaciones**\n",
    "\n",
    "---\n",
    "\n",
    "Mentoría #13 - Cómo hacer un Clasificador de Pliegos todoterreno (y de otros tipos de textos) usando NLP\n",
    "\n",
    "---\n",
    "Integrantes Grupo [1|2]:\n",
    "*   Bruce Wayne: bruce.w@wayne-entreprises.com\n",
    "*   Alfred Pennyworth: alfred@msn.com\n",
    "*   Richard Grayson: robin_132@gmail.com\n",
    "---\n",
    "\n",
    "Edición 2023\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "YGFbz1dvbFI2",
   "metadata": {
    "id": "YGFbz1dvbFI2"
   },
   "source": [
    "# Introducción\n",
    "\n",
    "En esta Mentoría, trabajaremos con un conjunto de datos que comprende aproximadamente 100.000 pliegos y licitaciones de diversos organismos nacionales, tanto públicos como privados. Estos datos se obtuvieron de un sistema/servicio diseñado para monitorear oportunidades de negocios, capturar la información en una base de datos, normalizarla y, posteriormente, clasificarla para informar a los usuarios según sus áreas de interés. Los usuarios de este sistema reciben alertas automáticas cada vez que se publica una oportunidad comercial que coincide con su perfil.\n",
    "\n",
    "## Desafío\n",
    "\n",
    "Como parte del proceso de clasificación, los pliegos se etiquetan utilizando principalmente reglas estáticas, como palabras clave, lo cual deja margen para optimizaciones. El título de cada pliego y principalmente la descripción de los objetos que se están licitando son campos de tipo texto y escritos por personas, por lo que naturalmente presentan ambigüedades y características propias del lenguaje que llevan a que un enfoque rígido y estático de clasificación, como el actual, resulte limitado y poco eficiente.\n",
    "\n",
    "El Desafío es utilizar las técnicas de Aprendizaje Automático, logrando así un \"Clasificador\" que utilice técnicas de Procesamiento de Lenguaje Natural (NLP) para clasificar de manera más eficiente en qué rubro o categoría se encuentra un pliego, basándose en su texto descriptivo y otros campos relacionados.\n",
    "\n",
    "## Interés General\n",
    "\n",
    "Más allá de la aplicación específica en este conjunto de datos, este problema, al estar vinculado al Procesamiento de Lenguaje Natural (PLN) y la clasificación, tiene la ventaja de poder ser utilizado posteriormente para otros tipos de contenido. De esta manera, el clasificador desarrollado podría aplicarse para categorizar libros, noticias, textos, tweets, publicaciones, artículos, etc.\n",
    "\n",
    "## Descripción del dataset\n",
    "\n",
    "A continuación se enumeran las diferentes variables del dataset, así como una breve descripción de su significado:\n",
    "\n",
    "- **id**: Clave única y primaria autoincremental de la tabla;\n",
    "- **cargado**: Fecha de carga del pliego;\n",
    "- **idexterno**: Id del pliego en la fuente;\n",
    "- **referencia**: Campo auxiliar obtenido de la fuente;\n",
    "- **objeto**: Campo principal, descripción del producto o servicio objeto de la licitación;\n",
    "- **rubro**: Campo de categorización disponible en la fuente. No siempre está disponible;\n",
    "- **agencia**: Empresa o Ente que lanza la licitación;\n",
    "- **apertura**: Fecha de apertura del pliego (vencimiento para presentarse al pliego);\n",
    "- **subrubro**: Subcategoría del pliego (también obtenido desde la fuente);\n",
    "- **pais**: País donde se lanza la licitación;\n",
    "- **observaciones**: Campo auxiliar donde se guardan datos extra que puede variar según la fuente;\n",
    "- **monto**: Monto del pliego, no siempre está publicado;\n",
    "- **divisaSimboloISO**: Moneda en la que se especifica el pliego;\n",
    "- **visible**: campo binario que determina si el pliego va a ser visualizado por el sistema (True/False);\n",
    "- **categoría**: Tipo de pliego, categorización entre diversos tipos de pliegos (Compra Directa, Licitación Simple, Subasta, etc.);\n",
    "- **fuente**: Fuente de donde se obtuvo la licitación."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaImQfxeYd8Z",
   "metadata": {
    "id": "eaImQfxeYd8Z"
   },
   "source": [
    "# Trabajo Práctico #2\n",
    "\n",
    "Como vimos en el primer Trabajo Práctico, en esta segunda entrega tenemos como objetivo preparar los datos para poder alimentar un modelo de predicción. Aplicaremos diferentes tipos de prácticas y estrategias para preparar el dataset y resolver todas las cuestiones detectadas en el TP1\n",
    "\n",
    "A continuación se enumeran los ejercicios propuestos como guía:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "EXILjx8pd2zb",
   "metadata": {
    "id": "EXILjx8pd2zb"
   },
   "source": [
    "## Ejercicio 1: Limpieza de Datos\n",
    "\n",
    "Según lo vimos en el TP1, aplicar las técnicas propuesta para limpiar los datos. En todos los casos, además de aplicar la técnica, redactar brevemente la explicación del criterio tomado.\n",
    "\n",
    "1.   Selección de Variables. Como vimos, hay variables que pueden descartarse.\n",
    "2.   Corrección de los diferentes Tipos de Variables. Aplicar las conversiones de tipo necesarias.\n",
    "3.   Selección de Registros. Filtrar las filas que no utilizaremos para la clasificación.\n",
    "4.   Curación de la variable [monto]. Tip: usar la variable [divisaSimboloISO]\n",
    "5.   Curación de la variable [fecha]. Incluye la eliminación de Outliers.\n",
    "6.   En [agencia], normalizar si es que existen entradas con valores equivalentes.\n",
    "7.   (Opcional) Para las variables categóricas aplicar técnicas de encoding, de manera de ir transformando variables de texto en números que los algoritmos puedan procesar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rpbtM0BGkWQ4",
   "metadata": {
    "id": "rpbtM0BGkWQ4"
   },
   "source": [
    "## Ejercicio 2: Variable [rubro]\n",
    "\n",
    "1.   Unificar categorías (rubros) equivalentes entre distintas fuentes. Usar un criterio definido por el grupo y explicarlo en pocas palabras.\n",
    "2.   Utilizar visualizaciones que permitan ver todas las categorías y verificar que sean valores válidos.\n",
    "3.   En caso de haber registros con categorías inválidas, tomar algún criterio para imputar la categoría o eliminar el registro. Desarrollar la decisión tomada.\n",
    "4.   Resolver si se va a utilizar subrubro para aperturar la categorización en algún caso. Definir luego qué postura tomar con respecto a la variable [rubro]\n",
    "5.   En las casos en que existe más de una categoría, tomar alguna postura para que los registros finalmente queden con una única etiqueta."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07xb2chXeBjx",
   "metadata": {
    "id": "07xb2chXeBjx"
   },
   "source": [
    "## Ejercicio 3: Variable [objeto]\n",
    "\n",
    "1.   Aplicar técnicas de lematización para eliminar artículos, conectores, stop_words en general. Quitar acentos (tildes), mayúsculas, plurales, caractéres inválidos, abreviaturas, etc., dejando solo palabras importantes para la clasificación.\n",
    "2.   Utilizar técnicas de visualización para detectar otros términos muy frecuentes que no aportan valor a la clasificación.\n",
    "3.   (Opcional) Aplicar técnicas de tokenización para convertir la variable objeto en un array de tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2i0GCYnYhQE6",
   "metadata": {
    "id": "2i0GCYnYhQE6"
   },
   "source": [
    "## Ejercicio 4: Exportación\n",
    "\n",
    "Con el dataset ya transformado, generar una versión final y exportarla a un nuevo archivo. Este es el resultado final del TP2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fDj77JylhG3_",
   "metadata": {
    "id": "fDj77JylhG3_"
   },
   "source": [
    "## Ejercicio 5: Análisis\n",
    "\n",
    "En base a todo lo aprendido en las materias obligatorias cursadas, qué tipo de modelos de clasificación piensan que podrían aplicarse al dataset obtenido? Desarrollar brevemente una justificación en comparación con otros modelos posibles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b56da7",
   "metadata": {
    "id": "14b56da7"
   },
   "source": [
    "# Desarrollo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5cec0d",
   "metadata": {},
   "source": [
    "# Librerías\n",
    "\n",
    "Importamos librerías necesarias para la visualización, y para desactivar warnings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "XbgaB6JqjjWy",
   "metadata": {
    "id": "XbgaB6JqjjWy"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import missingno as msno\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "#levantamos el dataset\n",
    "df = pd.read_csv(\"./datos/Mentoria_Dataset_0.csv\" , encoding='latin-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d2b6d5",
   "metadata": {},
   "source": [
    "# Filtrado\n",
    "\n",
    "Primero, hacemos la limpieza del dataset de acuerdo a lo que vimos en el TP1.\n",
    "\n",
    "Empezamos por quedarlos con los datos que tienen *categoría*=1, y las variables *referencia*, *objeto*, *rubro*, *agencia*, *apertura*, *monto* y *fuente*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "BMcav1-2aaJP",
   "metadata": {
    "id": "BMcav1-2aaJP"
   },
   "outputs": [],
   "source": [
    "#tiro los que tienen categoria 2\n",
    "df_fil = df[df['categoria']==1]\n",
    "#variables que decidimos relevantes\n",
    "variables = ['referencia', 'objeto', 'rubro','agencia','apertura','monto','fuente']\n",
    "df_fil = df_fil[variables]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94be04b0",
   "metadata": {},
   "source": [
    "# Limieza de las fechas\n",
    "\n",
    "Ahora hacemos la limpieza de las fechas que no tienen sentido:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2177b870",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hacemos el filtrado de fechas hecho en el práctico anterior:\n",
    "\n",
    "today_date = pd.Timestamp.today().date()\n",
    "min_date = pd.to_datetime('13/12/2015')\n",
    "df_fil.reset_index(inplace=True)  #reseteo indices, es util cuando indice=nro de fila\n",
    "\n",
    "#fechas malas que encontramos:\n",
    "fecha_mala1 = df_fil['apertura'][62]\n",
    "fecha_mala2 = df_fil['apertura'][2025]\n",
    "fecha_mala3 = df_fil['apertura'][11907]\n",
    "\n",
    "#removemos las fechas malas\n",
    "df_fil = df_fil.drop(df_fil[df_fil['apertura'] == fecha_mala1].index)\n",
    "df_fil = df_fil.drop(df_fil[df_fil['apertura'] == fecha_mala2].index)\n",
    "df_fil = df_fil.drop(df_fil[df_fil['apertura'] == fecha_mala3].index)\n",
    "\n",
    "#convierto a formato datetime\n",
    "df_fil['apertura'] = pd.to_datetime(df_fil['apertura'],format='%d/%m/%Y %H:%M')\n",
    "df_fil['apertura'] = df_fil['apertura'].dt.tz_localize(None)\n",
    "df_fil['apertura'] = df_fil['apertura'].dt.date\n",
    "\n",
    "#removemos outliers\n",
    "df_fil = df_fil.drop(df_fil[df_fil['apertura'] > today_date].index)\n",
    "df_fil = df_fil.drop(df_fil[pd.to_datetime(df_fil['apertura']) < min_date].index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089cf0cf",
   "metadata": {},
   "source": [
    "# Limpieza de la variable monto\n",
    "\n",
    "Limpiamos ahora la variable monto. En las entradas donde han NaN's, las definimos como 0. Además, convertimos dólares a peso y cambiamos strings a valores numéricos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d7564e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#precio del dolar oficial\n",
    "dolar_value = 255\n",
    "\n",
    "df_fil['monto'] = df_fil['monto'].replace(r'\\N', np.nan)\n",
    "df_fil['monto'] = df_fil['monto'].replace('0', 0)\n",
    "\n",
    "#remuevo las strings de espacios, pesos y dolares donde las haya. Cambio la representacion de string a float\n",
    "j=0\n",
    "for monto, i in zip(df_fil['monto'],df_fil.index):\n",
    "    if \" \" in str(monto):\n",
    "        df_fil['monto'][i] = str(df_fil['monto'][i]).replace(' ','')\n",
    "    if \"$\" in str(monto):\n",
    "        df_fil['monto'][i] = str(df_fil['monto'][i]).replace('$','')\n",
    "    if \".\" in str(monto):\n",
    "        df_fil['monto'][i] = str(df_fil['monto'][i]).replace('.','')\n",
    "    if \",\" in str(monto):\n",
    "        df_fil['monto'][i] = str(df_fil['monto'][i]).replace(',','.')    \n",
    "    if \"U$S\" in str(monto):\n",
    "        df_fil['monto'][i] = str(df_fil['monto'][i]).replace('US','')\n",
    "        #convierto a pesos y reemplazo la entrada\n",
    "        df_fil['monto'][i] = float(df_fil['monto'][i]) * dolar_value  \n",
    "        j+=1\n",
    "    df_fil['monto'][i] = float(df_fil['monto'][i]) \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55a3965",
   "metadata": {},
   "source": [
    "# Variable Objeto\n",
    "\n",
    "Ahora pasamos a procesar la variable *objeto*.\n",
    "\n",
    "Para empezar, importamos la librería `string` para trabajar con caracteres.\n",
    "Definimos tablas para hacer traducciones de caracteres no deseados, como por ejemplo todos los signos de puntuación, y números.\n",
    "\n",
    "No queremos que haya números en la columna *objeto* porque consideramos que tener, por ejemplo:\n",
    "\n",
    "* direcciones\n",
    "* palabras como '4x4'\n",
    "* precios\n",
    "* cantidad de productos o servicios\n",
    "\n",
    "no aportan nada al desarrollo de esta variable.\n",
    "\n",
    "Además, eliminamos tildes y pasamos todas las palabras a minúsculas.\n",
    "\n",
    "**Warning**: esta celda tarda mucho."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7e9c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "#tabla para reemplazar signos de puntuacion\n",
    "table_punct = str.maketrans(' ', ' ', string.punctuation)\n",
    "#tabla para reemplazar numeros\n",
    "table_num = str.maketrans(' ', ' ', '1234567890')\n",
    "table_otros = str.maketrans(' ', ' ', 'º°¿')\n",
    "\n",
    "#defino los datasets a limpiar\n",
    "df_test = df_fil[['objeto','rubro']]\n",
    "print(df_test['objeto'][0:100])\n",
    "\n",
    "df_test.reset_index(inplace=True)\n",
    "\n",
    "i=0\n",
    "for i in range (len(df_test)):    \n",
    "#for i in range (100):    \n",
    "    df_test['objeto'][i] = str(df_test['objeto'][i]).lower()\n",
    "    df_test['objeto'][i] = str(df_test['objeto'][i]).translate(table_punct) #elimino signos de puntuacion\n",
    "    df_test['objeto'][i] = str(df_test['objeto'][i]).translate(table_num) #elimino numeros\n",
    "    df_test['objeto'][i] = str(df_test['objeto'][i]).translate(table_otros) #elimino acentos y otros\n",
    "    \n",
    "    sent = str(df_test['objeto'][i])   # variable a analizar\n",
    "    if \"á\" in sent:\n",
    "        df_test['objeto'][i] = df_test['objeto'][i].replace('á','a')\n",
    "    if \"é\" in sent:\n",
    "        df_test['objeto'][i] = df_test['objeto'][i].replace('é','e')\n",
    "    if \"í\" in sent:\n",
    "        df_test['objeto'][i] = df_test['objeto'][i].replace('í','i')\n",
    "    if \"ó\" in sent:\n",
    "        df_test['objeto'][i] = df_test['objeto'][i].replace('ó','o')\n",
    "    if \"ú\" in sent:\n",
    "        df_test['objeto'][i] = df_test['objeto'][i].replace('ú','u')\n",
    "    if \"à\" in sent:\n",
    "        df_test['objeto'][i] = df_test['objeto'][i].replace('à','u')\n",
    "    if \"è\" in sent:\n",
    "        df_test['objeto'][i] = df_test['objeto'][i].replace('è','u')\n",
    "    if \"ì\" in sent:\n",
    "        df_test['objeto'][i] = df_test['objeto'][i].replace('ì','u')\n",
    "    if \"ò\" in sent:\n",
    "        df_test['objeto'][i] = df_test['objeto'][i].replace('ò','u')\n",
    "    if \"ù\" in sent:\n",
    "        df_test['objeto'][i] = df_test['objeto'][i].replace('ù','u')\n",
    "    i+=1\n",
    "    \n",
    "print(df_test['objeto'][0:100])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4330a23c",
   "metadata": {},
   "source": [
    "Ahora, pasamos a utilizar librerías específicas de PLN para tokenizar y lematizar las entradas del dataset.\n",
    "\n",
    "Para eso, usamos la librería `nltk`.\n",
    "\n",
    "`word_tokenize` es una función para convertir a una oración (una string larga) en una lista de sus palabras constituyentes. Cada entrada de dicha lista se denomina **token**.\n",
    "\n",
    "`stopwords` es una lista de palabras conectoras que no aportan nada de información en un modelo de PLN y que sólo aumentan la dimensionalidad del problema.\n",
    "\n",
    "`SnowballStemmer` Es la función que vamos a utilizar para convertir a cada palabra en su raíz, y así agrupamos muchas palabras que contengan la misma información (por ejemplo 'mantener' y 'mantenimiento').\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1f285b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "#funcion para lematizar una palabra\n",
    "def lemmatize_word(word):\n",
    "    stemmer = SnowballStemmer('spanish')\n",
    "    return stemmer.stem(word)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b34c6e",
   "metadata": {},
   "source": [
    "Vemos que en las 1000 primeras entradas del dataset, hay algún NaN que quedó dando vueltas que se convirtió en string. Removemos esas entradas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f555a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test0 = df_test.copy()\n",
    "\n",
    "print(df_test0['objeto'][:1000])\n",
    "\n",
    "#elimino algunos nans que aparecen\n",
    "df_test0 = df_test0.drop(df_test0[df_test0['objeto']=='nan'].index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28530ad6",
   "metadata": {},
   "source": [
    "Ahora tokenizamos la variable para trabajar mejor con elementos individuales:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c19770b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test1 = df_test0.copy()\n",
    "\n",
    "print('antes de tokenizar:')\n",
    "print()\n",
    "print(df_test1['objeto'][0:10])\n",
    "print('-'*100)\n",
    "print('después de tokenizar:')\n",
    "print()\n",
    "df_test1['objeto'] = df_test1['objeto'].apply(word_tokenize)\n",
    "print(df_test1['objeto'][:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57b205c",
   "metadata": {},
   "source": [
    "Para tener una idea de la distribución de palabras, definimos una función que devuelve una lista con la cantidad de veces que cada palabra se repite:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754207b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CountWords(df,column='objeto'):\n",
    "    '''\n",
    "    funcion para contar la ocurrencia de palabras en una columna de un dataset.\n",
    "    La columna ya tiene que venir tokenizada\n",
    "    '''\n",
    "    # Unimos las palabras lematizadas\n",
    "    df0 = df[column].apply(lambda words: ' '.join(words))\n",
    "    entire_words = \"\".join(' '+item for item in df0) #el espacio ' ' es para que no pegue palabras\n",
    "    list_ = entire_words.split()\n",
    "    df_list = pd.DataFrame(list_,columns=['palabras'])\n",
    "    counts = df_list.value_counts()\n",
    "\n",
    "    return counts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93103001",
   "metadata": {},
   "source": [
    "Utilicemos esta función para ver cómo v quedando nuestro dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40a126d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reseteo indices siempre después de remover entradas\n",
    "df_test1.reset_index(inplace=True)\n",
    "\n",
    "counts = CountWords(df_test1)\n",
    "print(counts[:1000])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a173fc7c",
   "metadata": {},
   "source": [
    "Hay un par de caracteres que aparecen en la lista que tienen que ser removidos. Veamos cómo queda:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e45359a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test2 = df_test0.copy()\n",
    "\n",
    "#elimino un caracter de espacio que se repite bastante\n",
    "for i in range(len(df_test2)):\n",
    "    if '\\x96' in  df_test2['objeto'][i]:\n",
    "        df_test2['objeto'][i] = df_test2['objeto'][i].replace('\\x96','')\n",
    "    if '\\x93' in  df_test0['objeto'][i]:\n",
    "        df_test2['objeto'][i] = df_test2['objeto'][i].replace('\\x93','')\n",
    "\n",
    "df_test2['objeto'] = df_test2['objeto'].apply(word_tokenize)\n",
    "\n",
    "counts = CountWords(df_test2)\n",
    "print(counts[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae088e62",
   "metadata": {},
   "source": [
    "Ahora, eliminamos stopwords y palabras que se repiten bastante, que creemos que no aportan nada al modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64620cb1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_test3 = df_test2.copy()\n",
    "print('Removemos stopwords:')\n",
    "print()\n",
    "stop_words = set(stopwords.words('spanish'))\n",
    "df_test3['objeto'] = df_test3['objeto'].apply(lambda words: [word for word in words if word not in stop_words])\n",
    "print(df_test3['objeto'][:100])\n",
    "print('*'*100)\n",
    "print('Removemos palabras que creemos que no aportan nada al dataset:')\n",
    "print()\n",
    "adq_list = ['adquisicion','adq','adquis', 'adquirir', 'adquisiion',\n",
    "            'contrato','contratacion', 'compra', 'provision',\n",
    "            'servicio','alquiler','alq','subasta', 'locacion',\n",
    "            'monto','estimado', 'renovacion']\n",
    "df_test3['objeto'] = df_test3['objeto'].apply(lambda words: [word for word in words if word not in adq_list])\n",
    "print(df_test3['objeto'][:100])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b184df1",
   "metadata": {},
   "source": [
    "Vemos que la distribución de palabras se va achicando y tienden a quedar palabras importantes. Sigue habiendo cosas que no estamos seguros de remover o no, como 'n' o 'er'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89dc8567",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = CountWords(df_test3)\n",
    "print(counts[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3cfab2",
   "metadata": {},
   "source": [
    "Finalmente, aplico la lematización para reducir las palabras a su raíz:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1232d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test4 = df_test3.copy()\n",
    "print('Aplicamos lematización a cada palabra')\n",
    "print()\n",
    "df_test4 = df_test4.copy()\n",
    "df_test4['objeto'] = df_test4['objeto'].apply(lambda words: [lemmatize_word(word) for word in words])\n",
    "print(df_test4['objeto'][:10])\n",
    "print('*'*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6023e811",
   "metadata": {},
   "source": [
    "Veamos la distribución de palabras lematizadas. Vemos que la lista cambia ya que algunas palabras distintas se agrupan en lo mismo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21c4055",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = CountWords(df_test4)\n",
    "print(counts[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de9e134",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "counts_lem = CountWords(df_test0_lem)\n",
    "counts_nolem = CountWords(df_test0)\n",
    "\n",
    "for i in range(1000):\n",
    "#    print(i,counts_lem.index[i],counts_lem.values[i])\n",
    "    print(i,counts_nolem.index[i],counts_nolem.values[i])\n",
    "#    print('-'*100)\n",
    "     \n",
    "# nwords = len(df_dum2)\n",
    "# print('cantidad de palabras en monto:',nwords)\n",
    "# print('cantidad de palabras distintas:',np.sum(counts))\n",
    "# print('las 1000 palabras más frecuentes son el '+str(round(np.sum(counts[:1000])/nwords*100,2))+'% del total')\n",
    "# print('las 2000 palabras más frecuentes son el '+str(round(np.sum(counts[:2000])/nwords*100,2))+'% del total')\n",
    "# print('las 5000 palabras más frecuentes son el '+str(round(np.sum(counts[:5000])/nwords*100,2))+'% del total')\n",
    "# print('las 10000 palabras más frecuentes son el '+str(round(np.sum(counts[:10000])/nwords*100,2))+'% del total')\n",
    "# print('las 20000 palabras más frecuentes son el '+str(round(np.sum(counts[:20000])/nwords*100,2))+'% del total')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc702370",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test1 = df_test0.copy()\n",
    "\n",
    "#viendo las 1000 palabras mas usadas, creemos que se pueden remover todoas estas palabras:\n",
    "a_remover_lst = ['er','año','años','ordoñez','trim','do','etc','pte','sg','to','x',\n",
    "'sa','grl','b','ii','nro','ra','at','usd','in','srl','expte','kg'\n",
    "'iii','rn','g','v','mm','hs','m','pn','t','et','av','xii','bv']\n",
    "\n",
    "for i in range(len(df_test1)):\n",
    "    list_ = df_test1['objeto'][i]\n",
    "    for word in list_:\n",
    "        if word in a_remover_lst:\n",
    "            df_test1['objeto'][i].remove(word)\n",
    "            \n",
    "counts2 = CountWords(df_test1)\n",
    "\n",
    "for i in range(1000):\n",
    "#    print(i,counts_lem.index[i],counts_lem.values[i])\n",
    "    print(i,counts2.index[i],counts2.values[i])\n",
    "#    print('-'*100)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
