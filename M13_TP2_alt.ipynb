{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "UtZBXCxdajr1",
   "metadata": {
    "id": "UtZBXCxdajr1"
   },
   "source": [
    "**Diplomatura en Ciencia de Datos, Aprendizaje Automático y sus Aplicaciones**\n",
    "\n",
    "---\n",
    "\n",
    "Mentoría #13 - Cómo hacer un Clasificador de Pliegos todoterreno (y de otros tipos de textos) usando NLP\n",
    "\n",
    "---\n",
    "Integrantes Grupo [1|2]:\n",
    "*   Bruce Wayne: bruce.w@wayne-entreprises.com\n",
    "*   Alfred Pennyworth: alfred@msn.com\n",
    "*   Richard Grayson: robin_132@gmail.com\n",
    "---\n",
    "\n",
    "Edición 2023\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "YGFbz1dvbFI2",
   "metadata": {
    "id": "YGFbz1dvbFI2"
   },
   "source": [
    "# Introducción\n",
    "\n",
    "En esta Mentoría, trabajaremos con un conjunto de datos que comprende aproximadamente 100.000 pliegos y licitaciones de diversos organismos nacionales, tanto públicos como privados. Estos datos se obtuvieron de un sistema/servicio diseñado para monitorear oportunidades de negocios, capturar la información en una base de datos, normalizarla y, posteriormente, clasificarla para informar a los usuarios según sus áreas de interés. Los usuarios de este sistema reciben alertas automáticas cada vez que se publica una oportunidad comercial que coincide con su perfil.\n",
    "\n",
    "## Desafío\n",
    "\n",
    "Como parte del proceso de clasificación, los pliegos se etiquetan utilizando principalmente reglas estáticas, como palabras clave, lo cual deja margen para optimizaciones. El título de cada pliego y principalmente la descripción de los objetos que se están licitando son campos de tipo texto y escritos por personas, por lo que naturalmente presentan ambigüedades y características propias del lenguaje que llevan a que un enfoque rígido y estático de clasificación, como el actual, resulte limitado y poco eficiente.\n",
    "\n",
    "El Desafío es utilizar las técnicas de Aprendizaje Automático, logrando así un \"Clasificador\" que utilice técnicas de Procesamiento de Lenguaje Natural (NLP) para clasificar de manera más eficiente en qué rubro o categoría se encuentra un pliego, basándose en su texto descriptivo y otros campos relacionados.\n",
    "\n",
    "## Interés General\n",
    "\n",
    "Más allá de la aplicación específica en este conjunto de datos, este problema, al estar vinculado al Procesamiento de Lenguaje Natural (PLN) y la clasificación, tiene la ventaja de poder ser utilizado posteriormente para otros tipos de contenido. De esta manera, el clasificador desarrollado podría aplicarse para categorizar libros, noticias, textos, tweets, publicaciones, artículos, etc.\n",
    "\n",
    "## Descripción del dataset\n",
    "\n",
    "A continuación se enumeran las diferentes variables del dataset, así como una breve descripción de su significado:\n",
    "\n",
    "- **id**: Clave única y primaria autoincremental de la tabla;\n",
    "- **cargado**: Fecha de carga del pliego;\n",
    "- **idexterno**: Id del pliego en la fuente;\n",
    "- **referencia**: Campo auxiliar obtenido de la fuente;\n",
    "- **objeto**: Campo principal, descripción del producto o servicio objeto de la licitación;\n",
    "- **rubro**: Campo de categorización disponible en la fuente. No siempre está disponible;\n",
    "- **agencia**: Empresa o Ente que lanza la licitación;\n",
    "- **apertura**: Fecha de apertura del pliego (vencimiento para presentarse al pliego);\n",
    "- **subrubro**: Subcategoría del pliego (también obtenido desde la fuente);\n",
    "- **pais**: País donde se lanza la licitación;\n",
    "- **observaciones**: Campo auxiliar donde se guardan datos extra que puede variar según la fuente;\n",
    "- **monto**: Monto del pliego, no siempre está publicado;\n",
    "- **divisaSimboloISO**: Moneda en la que se especifica el pliego;\n",
    "- **visible**: campo binario que determina si el pliego va a ser visualizado por el sistema (True/False);\n",
    "- **categoría**: Tipo de pliego, categorización entre diversos tipos de pliegos (Compra Directa, Licitación Simple, Subasta, etc.);\n",
    "- **fuente**: Fuente de donde se obtuvo la licitación."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaImQfxeYd8Z",
   "metadata": {
    "id": "eaImQfxeYd8Z"
   },
   "source": [
    "# Trabajo Práctico #2\n",
    "\n",
    "Como vimos en el primer Trabajo Práctico, en esta segunda entrega tenemos como objetivo preparar los datos para poder alimentar un modelo de predicción. Aplicaremos diferentes tipos de prácticas y estrategias para preparar el dataset y resolver todas las cuestiones detectadas en el TP1\n",
    "\n",
    "A continuación se enumeran los ejercicios propuestos como guía:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "EXILjx8pd2zb",
   "metadata": {
    "id": "EXILjx8pd2zb"
   },
   "source": [
    "## Ejercicio 1: Limpieza de Datos\n",
    "\n",
    "Según lo vimos en el TP1, aplicar las técnicas propuesta para limpiar los datos. En todos los casos, además de aplicar la técnica, redactar brevemente la explicación del criterio tomado.\n",
    "\n",
    "1.   Selección de Variables. Como vimos, hay variables que pueden descartarse.\n",
    "2.   Corrección de los diferentes Tipos de Variables. Aplicar las conversiones de tipo necesarias.\n",
    "3.   Selección de Registros. Filtrar las filas que no utilizaremos para la clasificación.\n",
    "4.   Curación de la variable [monto]. Tip: usar la variable [divisaSimboloISO]\n",
    "5.   Curación de la variable [fecha]. Incluye la eliminación de Outliers.\n",
    "6.   En [agencia], normalizar si es que existen entradas con valores equivalentes.\n",
    "7.   (Opcional) Para las variables categóricas aplicar técnicas de encoding, de manera de ir transformando variables de texto en números que los algoritmos puedan procesar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rpbtM0BGkWQ4",
   "metadata": {
    "id": "rpbtM0BGkWQ4"
   },
   "source": [
    "## Ejercicio 2: Variable [rubro]\n",
    "\n",
    "1.   Unificar categorías (rubros) equivalentes entre distintas fuentes. Usar un criterio definido por el grupo y explicarlo en pocas palabras.\n",
    "2.   Utilizar visualizaciones que permitan ver todas las categorías y verificar que sean valores válidos.\n",
    "3.   En caso de haber registros con categorías inválidas, tomar algún criterio para imputar la categoría o eliminar el registro. Desarrollar la decisión tomada.\n",
    "4.   Resolver si se va a utilizar subrubro para aperturar la categorización en algún caso. Definir luego qué postura tomar con respecto a la variable [rubro]\n",
    "5.   En las casos en que existe más de una categoría, tomar alguna postura para que los registros finalmente queden con una única etiqueta."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07xb2chXeBjx",
   "metadata": {
    "id": "07xb2chXeBjx"
   },
   "source": [
    "## Ejercicio 3: Variable [objeto]\n",
    "\n",
    "1.   Aplicar técnicas de lematización para eliminar artículos, conectores, stop_words en general. Quitar acentos (tildes), mayúsculas, plurales, caractéres inválidos, abreviaturas, etc., dejando solo palabras importantes para la clasificación.\n",
    "2.   Utilizar técnicas de visualización para detectar otros términos muy frecuentes que no aportan valor a la clasificación.\n",
    "3.   (Opcional) Aplicar técnicas de tokenización para convertir la variable objeto en un array de tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2i0GCYnYhQE6",
   "metadata": {
    "id": "2i0GCYnYhQE6"
   },
   "source": [
    "## Ejercicio 4: Exportación\n",
    "\n",
    "Con el dataset ya transformado, generar una versión final y exportarla a un nuevo archivo. Este es el resultado final del TP2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fDj77JylhG3_",
   "metadata": {
    "id": "fDj77JylhG3_"
   },
   "source": [
    "## Ejercicio 5: Análisis\n",
    "\n",
    "En base a todo lo aprendido en las materias obligatorias cursadas, qué tipo de modelos de clasificación piensan que podrían aplicarse al dataset obtenido? Desarrollar brevemente una justificación en comparación con otros modelos posibles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b56da7",
   "metadata": {
    "id": "14b56da7"
   },
   "source": [
    "# Desarrollo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "XbgaB6JqjjWy",
   "metadata": {
    "id": "XbgaB6JqjjWy"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import missingno as msno\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "df = pd.read_csv(\"./datos/Mentoria_Dataset_0.csv\" , encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "BMcav1-2aaJP",
   "metadata": {
    "id": "BMcav1-2aaJP"
   },
   "outputs": [],
   "source": [
    "#en el practico 1 decidimos tirar los datos que tienen categoría 2:\n",
    "#tiro los que tienen categoria 2\n",
    "df_fil = df[df['categoria']==1]\n",
    "\n",
    "#variables que decidimos relevantes el práctico pasado: \n",
    "variables = ['referencia', 'objeto', 'rubro','agencia','apertura','monto','fuente']\n",
    "\n",
    "#filtro el dataframe con estas variables:\n",
    "df_fil = df_fil[variables]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2177b870",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10294/218546107.py:4: UserWarning: Parsing dates in DD/MM/YYYY format when dayfirst=False (the default) was specified. This may lead to inconsistently parsed dates! Specify a format to ensure consistent parsing.\n",
      "  min_date = pd.to_datetime('13/12/2015')\n"
     ]
    }
   ],
   "source": [
    "#Hacemos el filtrado de fechas hecho en el práctico anterior:\n",
    "\n",
    "today_date = pd.Timestamp.today().date()\n",
    "min_date = pd.to_datetime('13/12/2015')\n",
    "df_fil.reset_index(inplace=True)  #reseteo indices, es util cuando indice=nro de fila\n",
    "\n",
    "#fechas malas que encontramos:\n",
    "fecha_mala1 = df_fil['apertura'][62]\n",
    "fecha_mala2 = df_fil['apertura'][2025]\n",
    "fecha_mala3 = df_fil['apertura'][11907]\n",
    "\n",
    "#removemos las fechas malas\n",
    "df_fil = df_fil.drop(df_fil[df_fil['apertura'] == fecha_mala1].index)\n",
    "df_fil = df_fil.drop(df_fil[df_fil['apertura'] == fecha_mala2].index)\n",
    "df_fil = df_fil.drop(df_fil[df_fil['apertura'] == fecha_mala3].index)\n",
    "\n",
    "#convierto a formato datetime\n",
    "df_fil['apertura'] = pd.to_datetime(df_fil['apertura'],format='%d/%m/%Y %H:%M')\n",
    "df_fil['apertura'] = df_fil['apertura'].dt.tz_localize(None)\n",
    "df_fil['apertura'] = df_fil['apertura'].dt.date\n",
    "\n",
    "#removemos outliers\n",
    "df_fil = df_fil.drop(df_fil[df_fil['apertura'] > today_date].index)\n",
    "df_fil = df_fil.drop(df_fil[pd.to_datetime(df_fil['apertura']) < min_date].index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5d7564e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10294/3566752287.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_fil['monto'][i] = float(df_fil['monto'][i])\n",
      "/tmp/ipykernel_10294/3566752287.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_fil['monto'][i] = str(df_fil['monto'][i]).replace(' ','')\n",
      "/tmp/ipykernel_10294/3566752287.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_fil['monto'][i] = str(df_fil['monto'][i]).replace('$','')\n",
      "/tmp/ipykernel_10294/3566752287.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_fil['monto'][i] = str(df_fil['monto'][i]).replace('.','')\n",
      "/tmp/ipykernel_10294/3566752287.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_fil['monto'][i] = str(df_fil['monto'][i]).replace(',','.')\n",
      "/tmp/ipykernel_10294/3566752287.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_fil['monto'][i] = str(df_fil['monto'][i]).replace('US','')\n",
      "/tmp/ipykernel_10294/3566752287.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_fil['monto'][i] = float(df_fil['monto'][i]) * dolar_value\n"
     ]
    }
   ],
   "source": [
    "#precio del dolar oficial\n",
    "dolar_value = 255\n",
    "\n",
    "df_fil['monto'] = df_fil['monto'].replace(r'\\N', np.nan)\n",
    "df_fil['monto'] = df_fil['monto'].replace('0', 0)\n",
    "\n",
    "#remuevo las strings de espacios, pesos y dolares donde las haya. Cambio la representacion de string a float\n",
    "j=0\n",
    "for monto, i in zip(df_fil['monto'],df_fil.index):\n",
    "    if \" \" in str(monto):\n",
    "        df_fil['monto'][i] = str(df_fil['monto'][i]).replace(' ','')\n",
    "    if \"$\" in str(monto):\n",
    "        df_fil['monto'][i] = str(df_fil['monto'][i]).replace('$','')\n",
    "    if \".\" in str(monto):\n",
    "        df_fil['monto'][i] = str(df_fil['monto'][i]).replace('.','')\n",
    "    if \",\" in str(monto):\n",
    "        df_fil['monto'][i] = str(df_fil['monto'][i]).replace(',','.')    \n",
    "    if \"U$S\" in str(monto):\n",
    "        df_fil['monto'][i] = str(df_fil['monto'][i]).replace('US','')\n",
    "        #convierto a pesos y reemplazo la entrada\n",
    "        df_fil['monto'][i] = float(df_fil['monto'][i]) * dolar_value  \n",
    "        j+=1\n",
    "    df_fil['monto'][i] = float(df_fil['monto'][i]) \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55a3965",
   "metadata": {},
   "source": [
    "Empiezo a hacer el ejercicio 3:\n",
    "\n",
    "a- Aplicar técnicas de lematización para eliminar artículos, conectores, stop_words en general. Quitar acentos (tildes), mayúsculas, plurales, caractéres inválidos, abreviaturas, etc., dejando solo palabras importantes para la clasificación.\n",
    "\n",
    "b- Utilizar técnicas de visualización para detectar otros términos muy frecuentes que no aportan valor a la clasificación.\n",
    "    \n",
    "c- (Opcional) Aplicar técnicas de tokenización para convertir la variable objeto en un array de tokens.\n",
    "\n",
    "\n",
    "donde aplicar la limpieza:\n",
    "* objeto\n",
    "* rubro\n",
    "* agencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7d7e9c76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/santiago/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/santiago/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "/tmp/ipykernel_10294/2929354565.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test['objeto'][i] = str(df_test['objeto'][i]).lower()\n",
      "/tmp/ipykernel_10294/2929354565.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test['objeto'][i] = str(df_test['objeto'][i]).translate(table) #elimino signos de puntuacion\n",
      "/tmp/ipykernel_10294/2929354565.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test['objeto'][i] = df_test['objeto'][i].replace('ó','o')\n",
      "/tmp/ipykernel_10294/2929354565.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test['objeto'][i] = df_test['objeto'][i].replace('é','e')\n",
      "/tmp/ipykernel_10294/2929354565.py:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test['objeto'][i] = df_test['objeto'][i].replace('í','i')\n",
      "/tmp/ipykernel_10294/2929354565.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test['objeto'][i] = df_test['objeto'][i].replace('á','a')\n",
      "/tmp/ipykernel_10294/2929354565.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test['objeto'][i] = df_test['objeto'][i].replace('ú','u')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                     adquisicion de bolsas de alimento\n",
      "1     subasta de 105 cabezas de ganado vacuno  3 lot...\n",
      "2     subasta de 455 cabezas de ganado vacuno  10 lo...\n",
      "3     subasta de 485 cabezas de ganado vacuno  7 lot...\n",
      "4     subasta de 280 cabezas de ganado vacuno  4 lot...\n",
      "5     subasta de 105 cabezas de ganado vacuno  3 lot...\n",
      "6     subasta de 245 cabezas de ganado vacuno  4 lot...\n",
      "7     adquisicion de repuestos e insumos para equipa...\n",
      "8     estudio sobre costos de produccion y precios d...\n",
      "9     obras de  construccion muelle de ushuaia y mue...\n",
      "10    obra mejoramiento del sistema de riego de anju...\n",
      "11     obra mejoramiento del sistema hidrico de angulos\n",
      "12                                                  nan\n",
      "13             adquisicion de minicargador y accesorios\n",
      "14                     subasta publica de ganado equino\n",
      "15                               venta de ganado equino\n",
      "16                          alimento  animales bioterio\n",
      "17    adquisicion de insumos para comedor   rubro ca...\n",
      "18    adquisicion de insumos para comedor   rubro fr...\n",
      "19    contratar la adquisicion de viveres necesarios...\n",
      "20    adquisicion de viveres para la intendencia nav...\n",
      "21    adquisicion de insumos de almacen   centro adu...\n",
      "22    adquisicion de filet de merluza para provision...\n",
      "23    adquisicion de agua envasada s especificacione...\n",
      "24    adquisicion facturas y pan pre cocidos congela...\n",
      "25     adquisicion de frutas   verduras   panificado...\n",
      "26                                adquisicion de huevos\n",
      "27    contratar un servicio de catering para vino de...\n",
      "28                   adquisicion de alimentos y bebida \n",
      "29    adquisicion de botellones de agua para proveer...\n",
      "30           servicio de catering para 15° dto misiones\n",
      "31    servicio de cantina para el personal de organi...\n",
      "32                adquisicion de alimetos para personas\n",
      "33                       adquisicion de bidones de agua\n",
      "34    adquirir el suministro de carne vacuna para la...\n",
      "35         adquisicion de verduras  frutas y hortalizas\n",
      "36                            compra de bidones de agua\n",
      "37    adquisicion de productos de almacen para el co...\n",
      "38    adquisicion de productos carnicos para el come...\n",
      "39                 adquisicion de  insumos de cafeteria\n",
      "40               adquisicion agua en  bidones de 20 lts\n",
      "41    adquisicion de viveres secos y lacteos para la...\n",
      "42    adquisicion de viveres especiales para el dist...\n",
      "43    adquisicion de agua potable  presentacion en b...\n",
      "44               adquisicion de bidones de agua mineral\n",
      "45    adquisicion de viveres frescos para la confecc...\n",
      "46               adquisicion de alimentos para personas\n",
      "47    adquisicion de ave especie pollo  congelado  p...\n",
      "48    adquirir alimentos para el personal para la co...\n",
      "49                              alimentos para personas\n",
      "50    adquisicion de racionamiento para detenidos ci...\n",
      "51    adquisicion de viveres frescos y secos para de...\n",
      "52                  servicio de racionamiento en cocido\n",
      "53               adquisicion de alimentos para personas\n",
      "54    adquisicion de frutas y verduras para su provi...\n",
      "55    adquisicion de viveres frescos y panificados p...\n",
      "56    locacion de un  1  inmueble para el enacom del...\n",
      "57                            servicio de fotocopiadora\n",
      "58                           locacion de fotocopiadoras\n",
      "59                           alquiler de fotocopiadoras\n",
      "60    locacion inmueble para destino sede oficina ca...\n",
      "61    locacion de inmueble para destino sede oficina...\n",
      "62                                  alquiler de oficina\n",
      "63    servicio de concesion de fotocopiadoras y libr...\n",
      "64    alquiler de dispensers de agua potable de red ...\n",
      "65    servicio de alquiler de 10 equipos fotocopiado...\n",
      "66    contratacion del servicio de alquiler de diez ...\n",
      "67    concesion de bar confiteria y kiosko de la fac...\n",
      "68    adquirir el servicio de alquiler de equipos de...\n",
      "69    esta convocatoria a formular ofertas se implem...\n",
      "70    alquiler instalacion y puesta en servicio de l...\n",
      "71                    servicio de comedor universitario\n",
      "72    alquiler de equipos fotocopiadores incluyendo ...\n",
      "73     adquisicion de colchones y almohadas ignifugos  \n",
      "74    adquisicion de articuilos para tareas de mante...\n",
      "75    adquisicion de ropa de cama para el alojamient...\n",
      "76    adquisicion de electrodomesticos  muebles y he...\n",
      "77       adquisicion  de  camas  cuchetas  para  bnus  \n",
      "78    adquisicion  de  colchones  y  almohadas  para...\n",
      "79    adquirir maquina industrial para el lavadero d...\n",
      "80    adquisicion de equipos de recreo y suministros...\n",
      "81                    adquisicion de aires frio   calor\n",
      "82                  adquisicion de un termotanque solar\n",
      "83    contratacion del seguro del parque automotor p...\n",
      "84    contratacion del seguro de riesgos del trabajo...\n",
      "85    contratacion de la cobertura del seguro de inc...\n",
      "86    contratacion del servicio de cobertura de segu...\n",
      "87    adquisicion de articulos de bazar para el inst...\n",
      "88                   adquisicion de articulos de bazar \n",
      "89    adquisicion de utensilios de cocina s especifi...\n",
      "90    adquisicion de vajillas para proveer a las int...\n",
      "91    adquisicion de articulos de bazar y menajes pa...\n",
      "92    adquisicion de articulos descartables para el ...\n",
      "93                       adquisicion de paneles de yeso\n",
      "94                                    tarimas de madera\n",
      "95    adquisicion de maderas e insumos de carpinteri...\n",
      "96     adquisicion de productos de vidrio  plasticos...\n",
      "97                        adquisicion de maderas varias\n",
      "98    contratacion de servicio de luch y ambientacio...\n",
      "99                 stands  mesas y sillas   su alquiler\n",
      "Name: objeto, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "table = str.maketrans(string.punctuation,' '*len(string.punctuation)) #tabla para eliminar signos de puntuacion\n",
    "\n",
    "\n",
    "#funcion para lematizar una palabra\n",
    "def lemmatize_word(word):\n",
    "    stemmer = SnowballStemmer('spanish')\n",
    "    return stemmer.stem(word)\n",
    "\n",
    "#defino los datasets a limpiar\n",
    "df_test = df_fil[['objeto','rubro']]\n",
    "\n",
    "df_test.reset_index(inplace=True)\n",
    "\n",
    "i=0\n",
    "for i in range (len(df_test)):    \n",
    "    df_test['objeto'][i] = str(df_test['objeto'][i]).lower()\n",
    "    df_test['objeto'][i] = str(df_test['objeto'][i]).translate(table) #elimino signos de puntuacion\n",
    "    sent = str(df_test['objeto'][i])   # variable a analizar\n",
    "    if \"á\" in sent:\n",
    "        df_test['objeto'][i] = df_test['objeto'][i].replace('á','a')\n",
    "    if \"é\" in sent:\n",
    "        df_test['objeto'][i] = df_test['objeto'][i].replace('é','e')\n",
    "    if \"í\" in sent:\n",
    "        df_test['objeto'][i] = df_test['objeto'][i].replace('í','i')\n",
    "    if \"ó\" in sent:\n",
    "        df_test['objeto'][i] = df_test['objeto'][i].replace('ó','o')\n",
    "    if \"ú\" in sent:\n",
    "        df_test['objeto'][i] = df_test['objeto'][i].replace('ú','u')\n",
    "    i+=1\n",
    "\n",
    "print(df_test['objeto'][0:100])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4b1f285b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/santiago/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/santiago/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/santiago/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizamos las oraciones en palabras\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTokenizamos las oraciones en palabras\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n\u001b[0;32m---> 12\u001b[0m df_test0[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobjeto\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf_test0\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mobjeto\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword_tokenize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(df_test0[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobjeto\u001b[39m\u001b[38;5;124m'\u001b[39m][:\u001b[38;5;241m10\u001b[39m])\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/core/series.py:4771\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4661\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4662\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4663\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4666\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4667\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4668\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4669\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4670\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4769\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4770\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/core/apply.py:1123\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[1;32m   1122\u001b[0m \u001b[38;5;66;03m# self.f is Callable\u001b[39;00m\n\u001b[0;32m-> 1123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/core/apply.py:1174\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1172\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1173\u001b[0m         values \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;241m.\u001b[39m_values\n\u001b[0;32m-> 1174\u001b[0m         mapped \u001b[38;5;241m=\u001b[39m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1175\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1176\u001b[0m \u001b[43m            \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1177\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1178\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1181\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1182\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/_libs/lib.pyx:2924\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/nltk/tokenize/__init__.py:129\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    115\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 129\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    131\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[1;32m    132\u001b[0m     ]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/nltk/tokenize/__init__.py:107\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03mReturn a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03musing NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m:param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    106\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m load(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizers/punkt/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlanguage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pickle\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 107\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/nltk/tokenize/punkt.py:1281\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer.tokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1277\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenize\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m, realign_boundaries: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m   1278\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1279\u001b[0m \u001b[38;5;124;03m    Given a text, returns a list of the sentences in that text.\u001b[39;00m\n\u001b[1;32m   1280\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1281\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msentences_from_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrealign_boundaries\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/nltk/tokenize/punkt.py:1341\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer.sentences_from_text\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msentences_from_text\u001b[39m(\n\u001b[1;32m   1333\u001b[0m     \u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m, realign_boundaries: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1334\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m   1335\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1336\u001b[0m \u001b[38;5;124;03m    Given a text, generates the sentences in that text by only\u001b[39;00m\n\u001b[1;32m   1337\u001b[0m \u001b[38;5;124;03m    testing candidate sentence breaks. If realign_boundaries is\u001b[39;00m\n\u001b[1;32m   1338\u001b[0m \u001b[38;5;124;03m    True, includes in the sentence closing punctuation that\u001b[39;00m\n\u001b[1;32m   1339\u001b[0m \u001b[38;5;124;03m    follows the period.\u001b[39;00m\n\u001b[1;32m   1340\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1341\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [text[s:e] \u001b[38;5;28;01mfor\u001b[39;00m s, e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_tokenize(text, realign_boundaries)]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/nltk/tokenize/punkt.py:1341\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msentences_from_text\u001b[39m(\n\u001b[1;32m   1333\u001b[0m     \u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m, realign_boundaries: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1334\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m   1335\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1336\u001b[0m \u001b[38;5;124;03m    Given a text, generates the sentences in that text by only\u001b[39;00m\n\u001b[1;32m   1337\u001b[0m \u001b[38;5;124;03m    testing candidate sentence breaks. If realign_boundaries is\u001b[39;00m\n\u001b[1;32m   1338\u001b[0m \u001b[38;5;124;03m    True, includes in the sentence closing punctuation that\u001b[39;00m\n\u001b[1;32m   1339\u001b[0m \u001b[38;5;124;03m    follows the period.\u001b[39;00m\n\u001b[1;32m   1340\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1341\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [text[s:e] \u001b[38;5;28;01mfor\u001b[39;00m s, e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_tokenize(text, realign_boundaries)]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/nltk/tokenize/punkt.py:1329\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer.span_tokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m realign_boundaries:\n\u001b[1;32m   1328\u001b[0m     slices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_realign_boundaries(text, slices)\n\u001b[0;32m-> 1329\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m slices:\n\u001b[1;32m   1330\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m (sentence\u001b[38;5;241m.\u001b[39mstart, sentence\u001b[38;5;241m.\u001b[39mstop)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/nltk/tokenize/punkt.py:1459\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer._realign_boundaries\u001b[0;34m(self, text, slices)\u001b[0m\n\u001b[1;32m   1446\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1447\u001b[0m \u001b[38;5;124;03mAttempts to realign punctuation that falls after the period but\u001b[39;00m\n\u001b[1;32m   1448\u001b[0m \u001b[38;5;124;03mshould otherwise be included in the same sentence.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1456\u001b[0m \u001b[38;5;124;03m    [\"(Sent1.)\", \"Sent2.\"].\u001b[39;00m\n\u001b[1;32m   1457\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1458\u001b[0m realign \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1459\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence1, sentence2 \u001b[38;5;129;01min\u001b[39;00m _pair_iter(slices):\n\u001b[1;32m   1460\u001b[0m     sentence1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mslice\u001b[39m(sentence1\u001b[38;5;241m.\u001b[39mstart \u001b[38;5;241m+\u001b[39m realign, sentence1\u001b[38;5;241m.\u001b[39mstop)\n\u001b[1;32m   1461\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sentence2:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/nltk/tokenize/punkt.py:321\u001b[0m, in \u001b[0;36m_pair_iter\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    319\u001b[0m iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(iterator)\n\u001b[1;32m    320\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 321\u001b[0m     prev \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/nltk/tokenize/punkt.py:1431\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer._slices_from_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m   1429\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_slices_from_text\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[\u001b[38;5;28mslice\u001b[39m]:\n\u001b[1;32m   1430\u001b[0m     last_break \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1431\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m match, context \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_match_potential_end_contexts(text):\n\u001b[1;32m   1432\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_contains_sentbreak(context):\n\u001b[1;32m   1433\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mslice\u001b[39m(last_break, match\u001b[38;5;241m.\u001b[39mend())\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/nltk/tokenize/punkt.py:1395\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer._match_potential_end_contexts\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m   1393\u001b[0m previous_slice \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mslice\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m   1394\u001b[0m previous_match \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1395\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m match \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_lang_vars\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mperiod_context_re\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfinditer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1396\u001b[0m \n\u001b[1;32m   1397\u001b[0m     \u001b[38;5;66;03m# Get the slice of the previous word\u001b[39;00m\n\u001b[1;32m   1398\u001b[0m     before_text \u001b[38;5;241m=\u001b[39m text[previous_slice\u001b[38;5;241m.\u001b[39mstop : match\u001b[38;5;241m.\u001b[39mstart()]\n\u001b[1;32m   1399\u001b[0m     index_after_last_space \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_last_whitespace_index(before_text)\n",
      "\u001b[0;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords') #creo que estamos haciendo lo mismo que al eliminar las preposiciones con esto\n",
    "nltk.download('wordnet')\n",
    "\n",
    "df_test0 = df_test\n",
    "\n",
    "print('Tokenizamos las oraciones en palabras')\n",
    "print()\n",
    "df_test0['objeto'] = df_test0['objeto'].apply(word_tokenize)\n",
    "print(df_test0['objeto'][:10])\n",
    "print('*'*100)\n",
    "print('Removemos stopwords (son como las contracciones?)')\n",
    "print()\n",
    "stop_words = set(stopwords.words('spanish'))\n",
    "df_test0['objeto'] = df_test0['objeto'].apply(lambda words: [word for word in words if word not in stop_words])\n",
    "print(df_test0['objeto'][:10])\n",
    "print('*'*100)\n",
    "print('Aplicamos lematización a cada palabra')\n",
    "print()\n",
    "df_test0['objeto'] = df_test0['objeto'].apply(lambda words: [lemmatize_word(word) for word in words])\n",
    "print(df_test0['objeto'][:10])\n",
    "print('*'*100)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# corpus = []\n",
    "# text_data = \"\"\n",
    "# for i in range(0, len(df_test)):\n",
    "#     text_data = str(df_test['objeto'][i])\n",
    "#     text_data = text_data.split()\n",
    "#     wl = WordNetLemmatizer()\n",
    "#     text_data = [wl.lemmatize(word) for word in text_data if not word in set(stopwords.words('spanish'))]\n",
    "#     text_data = ' '.join(text_data)\n",
    "#     corpus.append(text_data)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2de9e134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['adquisicion bolsas alimento', 'subasta 105 cabezas ganado vacuno (3 lotes 35 novillos gordos), establecimiento grl paz, ruta provincial nro 6 km 153,5, localidad ordoñez, provincia cordoba', 'subasta 455 cabezas ganado vacuno (10 lotes 35 novillos gordos c/u 3 lotes 35 vacas gordas c/u), establecimiento general urquiza, ruta provincial nro 6, sitio localidad arroyo cle, provincia rio', 'subasta 485 cabezas ganado vacuno (7 lotes 35 vacas invernada c/u 2 lotes 120 vaquillonas 2 3 años invernada c/u), establecimiento general avalos, 1ra seccion rural km 173, monte caseros, provincia corrientes', 'subasta 280 cabezas ganado vacuno (4 lotes 35 novillos gordos c/u, 2 lotes de55 vaquillonas 1 2 años invernada c/u, 1 lote 4 toros refugo, 1 lote 20 vacas conserva 1 lote 6 novillos refugo), establecimiento coronel pringles, s', 'subasta 105 cabezas ganado vacuno (3 lotes 35 novillos gordos), establecimiento general paz, ruta provincial nro 6 km 153,5, localidad ordoñez, provincia cordoba.', 'subasta 245 cabezas ganado vacuno (4 lotes 35 novillos gordos c/u 3 lotes 35 vacas gordas c/u), establecimiento general urquiza, ruta provincial nro 6, sitio localidad arroyo cle, provincia rios.', 'adquisicion repuestos insumos equipamiento jardineria', 'estudio costos produccion precios productos acuicolas', 'obras de: construccion muelle ushuaia muelle almanza', 'obra mejoramiento sistema riego anjullon', 'obra mejoramiento sistema hidrico angulos', 'nan', 'adquisicion minicargador accesorios', 'subasta publica ganado equino', 'venta ganado equino', 'alimento animales bioterio', 'adquisicion insumos comedor - rubro carnes - destinado comedor universitario posadas.', 'adquisicion insumos comedor - rubro frutas verduras - destinado comedor universitario posadas.', 'contratar adquisicion viveres necesarios, realizacion catering fin atender eventos protocolares realizan base naval puerto belgrano.-', 'adquisicion viveres intendencia naval ushuaia fin proveer zona ushuaia rio grande.', 'adquisicion insumos almacen - centro adulto mayor', 'adquisicion filet merluza provision area naval austral.', 'adquisicion agua envasada s/especificaciones - compulsa abreviada monto', 'adquisicion facturas pan pre-cocidos congelados consumir navegacion', '\"adquisicion frutas - verduras - panificado - helado, 4º trimestre 2017 1º. trimestre 2018\".', 'adquisicion huevos', 'contratar servicio catering vino honor ceremonia aniversario dia infanteria marina', 'adquisicion alimentos bebida.', 'adquisicion botellones agua proveer diversas oficinas depositos intendencia naval.-', 'servicio catering 15° dto misiones', 'servicio cantina personal organismos varios escuela suboficiales fuerza aerea centro regional universitario cordoba - iua', 'adquisicion alimetos persona', 'adquisicion bidones agua', 'adquirir suministro carne vacuna intendencia naval puerto belgrano', 'adquisicion verduras, frutas hortalizas', 'compra bidones agua', 'adquisicion productos almacen comedor universitario periodo tres me', 'adquisicion productos carnicos comedor universitario periodo tres me', 'adquisicion insumos cafeteria', 'adquisicion agua bidones 20 lts', 'adquisicion viveres secos lacteos confeccion menu hospitalizados', 'adquisicion viveres especiales distrito electoral chaco, correspondientes elecciones legislativas año 2017', 'adquisicion agua potable, presentacion bidones 20 litros cada parque nacional sierra quijadas', 'adquisicion bidones agua mineral', 'adquisicion viveres fresco confeccion menu hospitalizados', 'adquisicion alimentos persona', 'adquisicion ave especie pollo (congelado) consumo diario personal militar cuerpo cadetes liceo naval militar almirante storni', 'adquirir alimentos personal confeccion menu, reunion previa autoridades distrito electoral pampa elecciones legislativas 2017.', 'alimentos persona', 'adquisicion racionamiento detenidos civiles alojados unidad bidones agua envasadas x 5 lts', 'adquisicion viveres fresco secos dependencias subjefatura caecopaz', 'servicio racionamiento cocido', 'adquisicion alimentos persona', 'adquisicion frutas verduras provision intendencia naval ushuaia', 'adquisicion viveres fresco panificados intendencia naval ushuaia', 'locacion (1) inmueble enacom delegacion viedma', 'servicio fotocopiadora', 'locacion fotocopiadoras', 'alquiler fotocopiadoras', 'locacion inmueble destino sede oficina capitan sarmiento, provincia buenos aire', 'locacion inmueble destino sede oficina laprida, provincia buenos aire', 'alquiler oficina', 'servicio concesion fotocopiadoras libreria', 'alquiler dispenser agua potable red bidones periodo doce ( 12 meses) opcion prorroga doce (12 meses) mas.', 'servicio alquiler 10 equipos fotocopiadoras ina-ezeiza centros interior', 'contratacion servicio alquiler diez (10) impresoras fotocopiadoras laser blanco/negro multifuncion', 'concesion bar confiteria kiosko facultad humanidades ciencias sociales salud', 'adquirir servicio alquiler equipos computacion comunicacion afrontar tareas administrativas operacionales elecciones legislativas nacionales 2017.', 'convocatoria formular ofertas implementa obtener prestacion servicio alquiler equipamiento cada area laboratorio hospital naval puerto belgrano (secciones quimica clinica, inmunologia, virologia, endocr', 'alquiler instalacion puesta servicio central telefonica digital distribuidor general lineas hospital militar central', 'servicio comedor universitario', 'alquiler equipos fotocopiadores incluyendo insumos serv. tecnico termino 36 meses.', 'adquisicion colchones almohadas ignifugos.-', 'adquisicion articuilos tareas mantenimiento centro regional san miguel', 'adquisicion ropa cama alojamiento transito casa ss.oo., necesarios mantener servicio dicho alojamiento base aeronaval almirante zar', 'adquisicion electrodomesticos, muebles herramientas', 'adquisicion camas cuchetas bnus.-', 'adquisicion colchones almohadas bnus.-', 'adquirir maquina industrial lavadero escuela tecnicas tacticas navales', 'adquisicion equipos recreo suministros imprenta audiovisuales.', 'adquisicion aire frio - calor', 'adquisicion termotanque solar', 'contratacion seguro parque automotor ( 1 ) año.', 'contratacion seguro riesgos trabajo terminos ley n° 24.557 personal superintendencia riesgos trabajo', 'contratacion cobertura seguro incendio edificios item complementarios inmuebles parque nacional nahuel huapi', 'contratacion servicio cobertura seguros responsabilidad civil incendio edificio direccion regional noroeste', 'adquisicion articulos bazar instituto.', 'adquisicion articulos bazar.', 'adquisicion utensilios cocina s/especificaciones - compulsa breviada monto', 'adquisicion vajillas proveer intendencias navales unidades componentes armada argentina.', 'adquisicion articulos bazar menajes ejercicio ceibo hermandad año 2017.', 'adquisicion articulos descartables hospital general 601 - hospital militar central', 'adquisicion paneles yeso', 'tarimas madera', 'adquisicion maderas insumos carpinteria escuela naval militar.', '\"adquisicion productos vidrio, plasticos, carpinteria, ferreteria, sanitario, plomeria gas, htas pinturas\" \"para ix brigada aerea departamento contrataciones crv\"', 'adquisicion maderas varias', 'contratacion servicio luch ambientacion celebracion fin año 2017', 'stands, mesa sillas - alquiler']\n"
     ]
    }
   ],
   "source": [
    "print(corpus[0:100])"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
